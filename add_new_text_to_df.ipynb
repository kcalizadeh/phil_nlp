{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "add_new_text_to_df.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeK2Vk/FBO63G+7YHwOq31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcalizadeh/phil_nlp/blob/master/add_new_text_to_df.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjIY5RJiAtqF",
        "outputId": "f93a586c-f037-4fb2-90a2-9975f0b12a66"
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/gdrive',force_remount=True)\n",
        "\n",
        "drive_path = '/gdrive/MyDrive/Colab_Projects/Phil_NLP'\n",
        "\n",
        "sys.path.append(drive_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wf2kHOA2CtGn",
        "outputId": "0b1d0d31-d066-4192-ec1f-949e0aa09ec0"
      },
      "source": [
        "#@title imports\n",
        "!pip install symspellpy\n",
        "\n",
        "import re\n",
        "from google.colab import files\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell\n",
        "\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "# gets text from a gutenberg URL\n",
        "def get_guten(url):\n",
        "    # retrieve the source text\n",
        "    r = requests.get(url)\n",
        "    r.encoding = 'utf-8'\n",
        "    text = r.text\n",
        "    return text\n",
        "\n",
        "# gets the text from a txt file\n",
        "def get_text(path, encoding='utf-8'):\n",
        "    f = open(path, 'r', encoding=encoding)\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "    return text\n",
        "\n",
        "def baseline_clean(to_correct, capitals=True, bracketed_fn=False):\n",
        "  # remove utf8 encoding characters and some punctuations\n",
        "  result = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6§\\\\\\£\\Â‘’“”*_<>''\"\"⎫•{}]', ' ', to_correct)\n",
        "  result = re.sub(r'[\\u2014\\u2013\\u2012-]', ' ', result)\n",
        "\n",
        "  # replace whitespace characters with actual whitespace\n",
        "  result = re.sub(r'\\s', ' ', result)\n",
        "\n",
        "  # replace the ﬀ, ﬃ and ﬁ with the appropriate counterparts\n",
        "  result = re.sub(r'ﬀ', 'ff', result)\n",
        "  result = re.sub(r'ﬁ', 'fi', result)\n",
        "  result = re.sub(r'ﬃ', 'ffi', result)\n",
        "\n",
        "  # remove some recurring common and meaninless words/phrases\n",
        "  result = re.sub(r'\\s*This\\s*page\\s*intentionally\\s*left\\s*blank\\s*', ' ', result)\n",
        "  result = re.sub(r'(?i)Aufgabe\\s+', ' ', result)\n",
        "  result = re.sub(r',*\\s+cf\\.', ' ', result)\n",
        "  result = re.sub('coroll\\.', 'coroll', result)\n",
        "  result = re.sub('pt\\.', 'pt', result)\n",
        "\n",
        "  # some texts have footnotes conveniently in brackets - this removes them all, \n",
        "  # with a safety measure for unpaired brackets, and deletes all brackets afterwards\n",
        "  if bracketed_fn:\n",
        "    result = re.sub(r'\\[.{0,300}\\]|{.{0,300}}|{.{0,300}\\]|\\[.{0,300}}', ' ', result)\n",
        "  result = re.sub(r'[\\[\\]{}]', ' ', result)\n",
        "\n",
        "  # replace ampersands with 'and'\n",
        "  result = re.sub(r'&', 'and', result)\n",
        "\n",
        "  # remove roman numerals, first capitalized ones\n",
        "  result = re.sub(r'\\s((I{2,}V*X*\\.*)|(IV\\.*)|(IX\\.*)|(V\\.*)|(V+I*\\.*)|(X+L*V*I*]\\.*))\\s', ' ', result)\n",
        "  # then lowercase\n",
        "  result = re.sub(r'\\s((i{2,}v*x*\\.*)|(iv\\.*)|(ix\\.*)|(v\\.*)|(v+i*\\.*)|(x+l*v*i*\\.*))\\s', ' ', result)\n",
        "\n",
        "  # remove periods and commas flanked by numbers\n",
        "  result = re.sub(r'\\d\\.\\d', ' ', result)\n",
        "  result = re.sub(r'\\d,\\d', ' ', result)\n",
        "\n",
        "  # remove the number-letter-number pattern used for many citations\n",
        "  result = re.sub(r'\\d*\\w{,2}\\d', ' ', result)\n",
        "\n",
        "  # remove numerical characters\n",
        "  result = re.sub(r'\\d+', ' ', result)\n",
        "\n",
        "  # remove words of 2+ characters that are entirely capitalized \n",
        "  # (these are almost always titles, headings, or speakers in a dialogue)\n",
        "  # remove capital I's that follow capital words - these almost always roman numerals\n",
        "  # some texts do use these capitalizations meaningfully, so we make this optional\n",
        "  if capitals:\n",
        "    result = re.sub(r'[A-Z]{2,}\\s+I', ' ', result)\n",
        "    result = re.sub(r'[A-Z]{2,}', ' ', result)\n",
        "\n",
        "  # remove isolated colons and semicolons that result from removal of titles\n",
        "  result = re.sub(r'\\s+:\\s*', ' ', result)\n",
        "  result = re.sub(r'\\s+;\\s*', ' ', result)\n",
        "\n",
        "  # remove isolated letters (do it several times because strings of isolated letters do not get captured properly)\n",
        "  result = re.sub(r'\\s[^aAI\\.]\\s', ' ', result)\n",
        "  result = re.sub(r'\\s[^aAI\\.]\\s', ' ', result)\n",
        "  result = re.sub(r'\\s[^aAI\\.]\\s', ' ', result)\n",
        "  result = re.sub(r'\\s[^aAI\\.]\\s', ' ', result)\n",
        "  result = re.sub(r'\\s[^aAI\\.]\\s', ' ', result)\n",
        "\n",
        "  # remove isolated letters at the end of sentences or before commas\n",
        "  result = re.sub(r'\\s[^aI]\\.', '.', result)\n",
        "  result = re.sub(r'\\s[^aI],', ',', result)\n",
        "\n",
        "  # deal with spaces around periods and commas\n",
        "  result = re.sub(r'\\s,\\s', ', ', result)\n",
        "  result = re.sub(r'\\s\\.\\s;', '. ', result)\n",
        "\n",
        "  # remove empty parantheses\n",
        "  result = re.sub(r'(\\(\\s*\\.*\\s*\\))|(\\(\\s*,*\\s*)\\)', ' ', result)\n",
        "\n",
        "  # reduce multiple periods or whitespaces into a single one\n",
        "  result = re.sub(r'\\.+', '.', result)\n",
        "  result = re.sub(r'\\s+', ' ', result)\n",
        "\n",
        "  return result\n",
        "\n",
        "def remove_words(text, word_list):\n",
        "  for word in word_list:\n",
        "    text = re.sub(r''+word+'', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "def from_raw_to_df(text_dict):\n",
        "  nlp.max_length = 9000000\n",
        "  text = text_dict['text']\n",
        "  text = remove_words(text, text_dict['words to remove'])\n",
        "  text = baseline_clean(text, capitals=text_dict['remove capitals'],\n",
        "                        bracketed_fn=text_dict['bracketed fn'])\n",
        "  text_nlp = nlp(text, disable=['ner'])\n",
        "  text_df = pd.DataFrame(columns=['title', 'author', 'school', 'sentence_spacy'])\n",
        "  text_df['sentence_spacy'] = list(text_nlp.sents)\n",
        "  text_df['author'] = text_dict['author']\n",
        "  text_df['title'] = text_dict['title']\n",
        "  text_df['school'] = text_dict['school']\n",
        "  text_df['sentence_str'] = text_df['sentence_spacy'].apply(lambda x: ''.join(list(str(x))))\n",
        "  return text_df\n",
        "\n",
        "def space_words(str, \n",
        "                dict_path=pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\"),\n",
        "                edit_distance=1,\n",
        "                prefix_length=3):\n",
        "  sym_spell = SymSpell(max_dictionary_edit_distance=edit_distance, prefix_length=prefix_length)\n",
        "  sym_spell.load_dictionary(dict_path, term_index=0, count_index=1)\n",
        "\n",
        "  input_term = str\n",
        "  result = sym_spell.word_segmentation(input_term)\n",
        "\n",
        "  return result.corrected_string"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD2GQLRoC7NK"
      },
      "source": [
        "# load the existing csv so we can add to it\n",
        "main_df = pd.read_csv('phil_nlp.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBQd9Vl8EGRi"
      },
      "source": [
        "# load the text\n",
        "new_text = get_text('filepath')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kLRPvBpEOvv"
      },
      "source": [
        "# clip the front and end matter\n",
        "new_text = new_text.split('front')[1].split('end')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9H9oM7QEO8Z"
      },
      "source": [
        "# define terms that need to be removed ad hoc\n",
        "new_text_to_rm = ['headers', 'footers', 'oddities']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8K7X0o5EkeL"
      },
      "source": [
        "# build a dictionary for the text\n",
        "# be sure to mark if capitals should not be removed or if footnotes are bracketed\n",
        "new_text_dict = {'title': title,\n",
        "                 'author': author,\n",
        "                 'school': school,\n",
        "                 'text': new_text,\n",
        "                 'words to remove': new_text_to_rm,\n",
        "                 'remove capitals': True,\n",
        "                 'bracketed fn': False}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Ln9HsbFKcA"
      },
      "source": [
        "# turn the text into a dataframe\n",
        "new_text_df = from_raw_to_df(new_text_dict)\n",
        "\n",
        "len(new_text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "193nqYClFTRV"
      },
      "source": [
        "new_text_df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdQRI2nvFbZN"
      },
      "source": [
        "# examine short entries\n",
        "new_text_df['sentence_length'] = new_text_df['sentence_str'].map(lambda x: len(x))\n",
        "num_of_short_entries = len(new_text_df[new_text_df['sentence_length'] < 20])\n",
        "print(f\"there are {num_of_short_entries} so-called sentences with fewer than 20 characters\")\n",
        "new_text_df[new_text_df['sentence_length'] < 20].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7K-NfIIFqjm"
      },
      "source": [
        "# drop short entries if you choose\n",
        "new_text_df = new_text_df.drop(new_text_df[new_text_df['sentence_length'] < 20].index)\n",
        "len(new_text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roe4bijsF2Ku"
      },
      "source": [
        "# check for self-reference (especially if it is an older thinker)\n",
        "self_mention_df = new_text_df[new_text_df['sentence_lowered'].str\n",
        "                              .contains('\\s'+'author name'.lower())].copy()\n",
        "\n",
        "len(self_mention_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAvzCCaBGSq3"
      },
      "source": [
        "# preview to see if they seem problematic\n",
        "self_mention_df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf_S7DO5GYvG"
      },
      "source": [
        "# drop self-referring entries\n",
        "new_text_df = new_text_df.drop(new_text_df[new_text_df['sentence_lowered'].str\n",
        "                                           .contains('/s' + 'author name'.lower())].index)\n",
        "\n",
        "len(new_text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZbI9DhrGshM"
      },
      "source": [
        "# check the number of duplicates\n",
        "len(new_text_df['sentence_str'])-len(new_text_df['sentence_str'].drop_duplicates())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uTz_eDpGzWD"
      },
      "source": [
        "# examine the duplicate entries\n",
        "doubles_df = pd.concat(g for _, g in new_text_df.groupby(\"sentence_str\") if len(g) > 1)\n",
        "doubles_df.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6JORHyFG6AI"
      },
      "source": [
        "# drop the duplicates\n",
        "# this defaults to dropping both copies of duplicated rows\n",
        "new_text_df = new_text_df.drop(new_text_df['sentence_str'].duplicated(keep=False))].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6BvWHOhHN0T"
      },
      "source": [
        "# you're almost done! merge with the main dataframe \n",
        "main_df = main_df.append(new_text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVgUaXxyHpYf"
      },
      "source": [
        "# double check the length to make sure all is well\n",
        "len(main_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShWLvmJXHt7M"
      },
      "source": [
        "# if you're confident all went well, re-export\n",
        "# be sure, since this will overwrite the old file\n",
        "main_df.to_csv('phil_nlp.csv') \n",
        "files.download('phil_nlp.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}