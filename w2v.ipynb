{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fRqjeaLlmZHe",
        "_7J7_WOz2VK8"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOFk9nf54Omggt8Tdqo3wII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcalizadeh/phil_nlp/blob/master/w2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxG_6U1lGEa"
      },
      "source": [
        "### Imports and Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRT6BjwbiM8n",
        "outputId": "fcf6254b-cc46-4f2f-85d2-b191e774a2e8"
      },
      "source": [
        "# this cell mounts drive, sets the correct directory, then imports all functions\n",
        "# and relevant libraries via the functions.py file\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "# install relevent libraries not included with colab\n",
        "!pip install lime\n",
        "!pip install symspellpy\n",
        "\n",
        "drive.mount('/gdrive',force_remount=True)\n",
        "\n",
        "drive_path = '/gdrive/MyDrive/Colab_Projects/Phil_NLP'\n",
        "\n",
        "sys.path.append(drive_path)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.6/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (7.0.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (1.0.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.19.5)\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_FSrsN8l0J4",
        "outputId": "b982f19f-4f06-44cf-b1d7-33b4cddb8cc2"
      },
      "source": [
        "from functions import *\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random_seed=17"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqjeaLlmZHe"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "PHxy7346l3hZ",
        "outputId": "d0014088-7bb7-4e6e-8426-4353b40799f7"
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Colab_Projects/Phil_NLP/phil_nlp.csv')\n",
        "\n",
        "df.sample(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>school</th>\n",
              "      <th>sentence_spacy</th>\n",
              "      <th>sentence_str</th>\n",
              "      <th>sentence_length</th>\n",
              "      <th>sentence_lowered</th>\n",
              "      <th>lemmatized_str</th>\n",
              "      <th>tokenized_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53117</th>\n",
              "      <td>complete works vol 1</td>\n",
              "      <td>Aristotle</td>\n",
              "      <td>aristotle</td>\n",
              "      <td>what the thinking power is, or the perceptive,...</td>\n",
              "      <td>what the thinking power is, or the perceptive,...</td>\n",
              "      <td>211</td>\n",
              "      <td>what the thinking power is, or the perceptive,...</td>\n",
              "      <td>what the thinking power be , or the perceptiv...</td>\n",
              "      <td>['what', 'the', 'thinking', 'power', 'is', ','...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123397</th>\n",
              "      <td>search after truth</td>\n",
              "      <td>Malebranche</td>\n",
              "      <td>rationalism</td>\n",
              "      <td>and His immutable will.</td>\n",
              "      <td>and His immutable will.</td>\n",
              "      <td>23</td>\n",
              "      <td>and his immutable will.</td>\n",
              "      <td>and -PRON- immutable will .</td>\n",
              "      <td>['and', 'His', 'immutable', 'will', '.']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318166</th>\n",
              "      <td>political economy</td>\n",
              "      <td>Ricardo</td>\n",
              "      <td>capitalism</td>\n",
              "      <td>An extraordinary stimulus may be also given fo...</td>\n",
              "      <td>An extraordinary stimulus may be also given fo...</td>\n",
              "      <td>388</td>\n",
              "      <td>an extraordinary stimulus may be also given fo...</td>\n",
              "      <td>an extraordinary stimulus may be also give fo...</td>\n",
              "      <td>['An', 'extraordinary', 'stimulus', 'may', 'be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95555</th>\n",
              "      <td>essay concerning human understanding bk 2</td>\n",
              "      <td>Locke</td>\n",
              "      <td>empiricism</td>\n",
              "      <td>Contrary testimonies.</td>\n",
              "      <td>Contrary testimonies.</td>\n",
              "      <td>21</td>\n",
              "      <td>contrary testimonies.</td>\n",
              "      <td>contrary testimony .</td>\n",
              "      <td>['Contrary', 'testimonies', '.']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118901</th>\n",
              "      <td>search after truth</td>\n",
              "      <td>Malebranche</td>\n",
              "      <td>rationalism</td>\n",
              "      <td>It must therefore be concluded from the experi...</td>\n",
              "      <td>It must therefore be concluded from the experi...</td>\n",
              "      <td>372</td>\n",
              "      <td>it must therefore be concluded from the experi...</td>\n",
              "      <td>-PRON- must therefore be conclude from the ex...</td>\n",
              "      <td>['It', 'must', 'therefore', 'be', 'concluded',...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            title  ...                                      tokenized_txt\n",
              "53117                        complete works vol 1  ...  ['what', 'the', 'thinking', 'power', 'is', ','...\n",
              "123397                         search after truth  ...           ['and', 'His', 'immutable', 'will', '.']\n",
              "318166                          political economy  ...  ['An', 'extraordinary', 'stimulus', 'may', 'be...\n",
              "95555   essay concerning human understanding bk 2  ...                   ['Contrary', 'testimonies', '.']\n",
              "118901                         search after truth  ...  ['It', 'must', 'therefore', 'be', 'concluded',...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR-d0hRI7hmQ"
      },
      "source": [
        "# using gensim's built-in tokenizer \r\n",
        "df['gensim_tokenized'] = df['sentence_str'].map(lambda x: simple_preprocess(x.lower(),deacc=True,\r\n",
        "                                                        max_len=100))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-mTBCSDojS1",
        "outputId": "6e98acb9-5a68-47c2-9c60-e942a01be907"
      },
      "source": [
        "# check how it worked\r\n",
        "print(df.iloc[216066]['sentence_str'])\r\n",
        "df['gensim_tokenized'][216066]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The bumble bee is a part of the reproductive system of the clover.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'bumble',\n",
              " 'bee',\n",
              " 'is',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'reproductive',\n",
              " 'system',\n",
              " 'of',\n",
              " 'the',\n",
              " 'clover']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KxV2JTIoyxE"
      },
      "source": [
        "Well with that beautiful little quote, we are ready to start training our w2v model! At first we'll focus on a single school, since a single school is more likely to have consistency in their use of a word.\r\n",
        "\r\n",
        "Unfortunately, we didn't have much luck with just training on the texts alone. The code for it is left here for posterity, but it was when we worked with GloVe as the base that we had results that were actually useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th5FA84Ip-ha"
      },
      "source": [
        "### Word 2 Vec Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cv5UgQEqEGK"
      },
      "source": [
        "#### German Idealism as a Test Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFuFkbN7qN2w"
      },
      "source": [
        "We start by examining the texts of German Idealism to get a feel for what kind of parameters would work best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78GJHWsMF2he"
      },
      "source": [
        "def make_w2v(series, stopwords=[], size=200, window=5, min_count=5, workers=-1, \r\n",
        "             epochs=20, lowercase=True, sg=0, seed=17, cbow_mean=1, alpha=0.025,\r\n",
        "             sample=0.001, use_bigrams=True, threshold=10, bigram_min=5):\r\n",
        "  # turn the series into a list, lower it, clean it\r\n",
        "    sentences = [sentence for sentence in series]\r\n",
        "    if lowercase:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "    else:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  # incorporate bigrams\r\n",
        "    if use_bigrams:\r\n",
        "      bigram = Phrases(cleaned, min_count=bigram_min, threshold=threshold, delimiter=b' ')\r\n",
        "      bigram_phraser = Phraser(bigram)\r\n",
        "      tokens_list = []\r\n",
        "      for sent in cleaned:\r\n",
        "        tokens_ = bigram_phraser[sent]\r\n",
        "        tokens_list.append(tokens_)\r\n",
        "      cleaned = tokens_list\r\n",
        "    else:\r\n",
        "      cleaned = cleaned\r\n",
        "\r\n",
        "  # build the model\r\n",
        "    model = Word2Vec(cleaned, size=size, window=window, \r\n",
        "                     min_count=min_count, workers=workers, seed=seed, sg=sg,\r\n",
        "                     cbow_mean=cbow_mean, alpha=alpha, sample=sample)\r\n",
        "    model.train(series, total_examples=model.corpus_count, epochs=epochs)\r\n",
        "    model_wv = model.wv\r\n",
        "    \r\n",
        "  # clear it to avoid unwanted transference\r\n",
        "    del model\r\n",
        "\r\n",
        "    return model_wv"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQLm1mhOO6w"
      },
      "source": [
        "gi_wv = make_w2v(df[df['school'] == 'german_idealism']['gensim_tokenized'], threshold=12)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUf7kAzvrh7K"
      },
      "source": [
        "We can check this model by trying out a few words. For that purpose we have a testing function that tries some common word combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMJat1b1rhwK"
      },
      "source": [
        "pairs_to_try = [(['law', 'moral'], []),\r\n",
        "                (['self', 'consciousness'], []),\r\n",
        "                (['dialectic'], []),\r\n",
        "                (['logic'], []),\r\n",
        "]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXOnjOLpOcxi",
        "outputId": "cd22b349-46f7-4da9-820f-a9d0067abb53"
      },
      "source": [
        "test_w2v_pos_neg(gi_wv, pairs_to_try)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- aesthetically (0.24603)\n",
            "- bodily (0.24569)\n",
            "- inverse ratio (0.23537)\n",
            "- new (0.22829)\n",
            "- synonymous (0.22511)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- corresponding (0.25857)\n",
            "- que (0.25134)\n",
            "- patriarchal (0.24172)\n",
            "- rhythm (0.22662)\n",
            "- objecta (0.2262)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- situations (0.24978)\n",
            "- intelligible substrate (0.24178)\n",
            "- give rise (0.24007)\n",
            "- endlessly (0.23979)\n",
            "- indianapolis ind (0.22153)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- vaguely (0.2435)\n",
            "- events (0.23107)\n",
            "- projects (0.22523)\n",
            "- religion (0.22428)\n",
            "- indepen (0.22402)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgu0ee-frhZQ"
      },
      "source": [
        "Although some of these make a modicum of sense a lot of them seem like just gibberish. Let's try messing with some parameters.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS8zfzZHrhNe"
      },
      "source": [
        "##### Trying Skip-gram instead of C-bow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiV-E9xzrhCL"
      },
      "source": [
        "# make a base model with the preset parameters\r\n",
        "skip_gi_wv = make_w2v(series = df[df['school'] == 'german_idealism']['gensim_tokenized'], \r\n",
        "                         stopwords=[], sg=1, seed=0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS_Jzbgirg1y",
        "outputId": "bef02eb4-cef4-407b-919d-3d235c65324d"
      },
      "source": [
        "test_w2v(skip_gi_wv, pairs_to_try)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- mistakes (0.21409)\n",
            "- do (0.2132)\n",
            "- remaining (0.21286)\n",
            "- ingredient (0.21154)\n",
            "- unceasing (0.21109)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- ultimate decision (0.28703)\n",
            "- perfected (0.26523)\n",
            "- despite (0.24827)\n",
            "- loc (0.22929)\n",
            "- education (0.22864)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- transcendental use (0.28578)\n",
            "- indivisible (0.24304)\n",
            "- communicate (0.23995)\n",
            "- unconscious (0.23945)\n",
            "- ethi (0.22595)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- playing (0.28748)\n",
            "- continued (0.27647)\n",
            "- intentions (0.24647)\n",
            "- substratum (0.23162)\n",
            "- fettered (0.23124)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnNnqhueyaK-"
      },
      "source": [
        "These seem mildy more sensible. Let's tweak the other parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAv1PjLD7ZSh"
      },
      "source": [
        "##### Parameter Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Y3AggQyaVD",
        "outputId": "a9b263a3-d845-4816-e453-edd6d7572c5b"
      },
      "source": [
        "model_v1 = make_w2v(df[df['school'] == 'german_idealism']['gensim_tokenized'],\r\n",
        "                       stopwords=[],\r\n",
        "                       size=500,\r\n",
        "                       window=5,\r\n",
        "                       min_count=25,\r\n",
        "                       epochs=10,\r\n",
        "                       sg=1, \r\n",
        "                       seed=45)\r\n",
        "\r\n",
        "len(model_v1.vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVVqIAf8yadZ",
        "outputId": "f1bb4108-c3d6-4552-bb96-8a45ec595229"
      },
      "source": [
        "test_w2v(model_v1, pairs_to_try)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- positively (0.154)\n",
            "- association (0.14129)\n",
            "- demand (0.13406)\n",
            "- toward (0.12594)\n",
            "- legislative (0.11773)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- absence (0.1558)\n",
            "- natural science (0.15302)\n",
            "- wants (0.13851)\n",
            "- instinct (0.13707)\n",
            "- commands (0.13615)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- assuming (0.18621)\n",
            "- inwardly (0.16957)\n",
            "- sublating (0.15569)\n",
            "- disappears (0.14731)\n",
            "- comprehension (0.14721)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- emerge (0.14531)\n",
            "- range (0.14274)\n",
            "- quantum (0.13392)\n",
            "- outer (0.13161)\n",
            "- clearly (0.12611)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn8zvu2z2U_o"
      },
      "source": [
        "Despite tweaking parameters far and wide, it's difficult to get any results that are compellingly sensible. In most cases there are one or two terms in the similarity list that make some sense but others that are just strange or unconnected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7J7_WOz2VK8"
      },
      "source": [
        "#### Trying Another School"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZdUtK642VcE",
        "outputId": "99d6772d-7029-4fbd-d6e2-62f2c51e3bbb"
      },
      "source": [
        "cm_w2v = make_w2v(df[df['school'] == 'communism']['gensim_tokenized'],\r\n",
        "                       stopwords=[],\r\n",
        "                       size=700,\r\n",
        "                       window=10,\r\n",
        "                       min_count=10,\r\n",
        "                       epochs=25,\r\n",
        "                       sg=1, \r\n",
        "                       seed=10)\r\n",
        "\r\n",
        "type(cm_w2v)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gensim.models.keyedvectors.Word2VecKeyedVectors"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYoO0ozP2VmK"
      },
      "source": [
        "pairs_to_try=[(['material', 'conditions'], []),\r\n",
        "              (['worker'], ['owner']),\r\n",
        "              (['alienation', 'labor'], []),\r\n",
        "              (['capital'], [])]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1809IvVx2Vuy",
        "outputId": "f9f1cd87-afdb-4eb6-bd2c-90fd301755e6"
      },
      "source": [
        "test_w2v(cm_w2v, pairs_to_try)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['material', 'conditions']\tNegative - []\n",
            "- export (0.12331)\n",
            "- points (0.11569)\n",
            "- consumption (0.1128)\n",
            "- man (0.11206)\n",
            "- re appears (0.11086)\n",
            "\n",
            "Positive - ['worker']\tNegative - ['owner']\n",
            "- bernsteinism (0.11703)\n",
            "- accumulation (0.11645)\n",
            "- processes (0.11398)\n",
            "- brings about (0.1123)\n",
            "- agitators (0.1074)\n",
            "\n",
            "Positive - ['alienation', 'labor']\tNegative - []\n",
            "- succession (0.13823)\n",
            "- clearly (0.13428)\n",
            "- intellectual (0.12619)\n",
            "- advantage (0.12468)\n",
            "- fairly (0.12392)\n",
            "\n",
            "Positive - ['capital']\tNegative - []\n",
            "- variable part (0.1325)\n",
            "- draft (0.12887)\n",
            "- historical (0.12735)\n",
            "- seem (0.12625)\n",
            "- industrial reserve (0.11934)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hUYWlYr2V3L"
      },
      "source": [
        "Here the results were similar - a few words that made some sense and plenty that were just odd.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5ZnODM2V_P"
      },
      "source": [
        "### Transfer Learning with GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAOaelq2WIr"
      },
      "source": [
        "We'll import GloVe vectors as w2v, then use those as a base from which to train new vectors that are tuned to our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcFZMRT82WRC"
      },
      "source": [
        "glove_file = datapath('/gdrive/MyDrive/Colab_Projects/Phil_NLP/glove.6B.50d.txt')\r\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\r\n",
        "\r\n",
        "_ = glove2word2vec(glove_file, tmp_file)\r\n",
        "\r\n",
        "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hPyZ2aHugg"
      },
      "source": [
        "pairs_to_try = [(['law', 'moral'], []),\r\n",
        "                (['self', 'consciousness'], []),\r\n",
        "                (['dialectic'], []),\r\n",
        "                (['logic'], []),\r\n",
        "]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjdDWtd72Tli",
        "outputId": "2c06141e-98f5-4079-a6f9-b5fd615d2c03"
      },
      "source": [
        "# check out how GloVe works on our test pairs\r\n",
        "test_w2v_pos_neg(glove_vectors, pairs_to_try)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- morality (0.82654)\n",
            "- legal (0.82652)\n",
            "- laws (0.81529)\n",
            "- constitutional (0.80616)\n",
            "- fundamental (0.80217)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- sense (0.83446)\n",
            "- mind (0.79755)\n",
            "- vision (0.78202)\n",
            "- belief (0.78031)\n",
            "- life (0.77984)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- hegelian (0.88376)\n",
            "- dialectical (0.83417)\n",
            "- dialectics (0.80672)\n",
            "- materialist (0.77674)\n",
            "- metaphysics (0.77488)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- reasoning (0.81405)\n",
            "- intuitionistic (0.76531)\n",
            "- concepts (0.75831)\n",
            "- logical (0.75604)\n",
            "- theory (0.75026)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFyauZoo7KW"
      },
      "source": [
        "Ok these make a lot more sense right from the start. But we want them to be trained on our actual philosophical texts - that way we can see how different thinkers use different words and potentially use the vectors for classification.\r\n",
        "\r\n",
        "So in the cells below we train the existing GloVe model on on the German Idealist texts as a test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSOA8Zz2o7vX"
      },
      "source": [
        "# isolate the relevant school\r\n",
        "documents = df[df['school'] == 'german_idealism']['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "base_model = Word2Vec(size=300, min_count=5)\r\n",
        "base_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = base_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "base_model.train(trigramed_tokens, total_examples=total_examples, epochs=base_model.epochs)\r\n",
        "base_model_wv = base_model.wv\r\n",
        "del base_model"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0FjXahncQya",
        "outputId": "5064a66f-73f4-4cc2-ffc5-a05f32b058df"
      },
      "source": [
        "test_w2v(base_model_wv, pairs_to_try)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- freedom (0.82361)\n",
            "- morality (0.79375)\n",
            "- rule (0.78852)\n",
            "- moral law (0.78502)\n",
            "- happiness (0.78457)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- self consciousness (0.92599)\n",
            "- essence (0.87534)\n",
            "- objectivity (0.85972)\n",
            "- externality (0.85939)\n",
            "- immediacy (0.85824)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- antinomy (0.89444)\n",
            "- doctrine (0.89214)\n",
            "- remainder (0.88909)\n",
            "- resolution (0.87126)\n",
            "- procedure (0.86174)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- doctrine (0.84756)\n",
            "- pure reason (0.84016)\n",
            "- science (0.82024)\n",
            "- metaphysics (0.81265)\n",
            "- idealism (0.80801)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3MnHFbcl7d"
      },
      "source": [
        "We can immediately see that these make a lot more sense (and the similarity scores are a lot higher). 'Self' + 'consciousness' is rightly associated with 'self consciousness' and 'moral' + 'law' with 'moral law'. It even identifies the German Idealist tendency to unify logic and metaphysics. \r\n",
        "\r\n",
        "This is a massive improvement - these vectors can be fairly said to reflect how german idealists use these terms. Moreover, they are significantly different than the original GloVe model, which indicates that there was real learning going on here.\r\n",
        "\r\n",
        "For comparison, let's check these same terms, but as used by Phenomenologists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1Jkkc7dqk9"
      },
      "source": [
        "def train_glove(school, glove_vectors, threshold=10, stopwords=[],\r\n",
        "                min_count=20):\r\n",
        "  # isolate the relevant school\r\n",
        "  documents = df[df['school'] ==school]['gensim_tokenized']\r\n",
        "\r\n",
        "  # format the series to be used\r\n",
        "  stopwords = []\r\n",
        "\r\n",
        "  sentences = [sentence for sentence in documents]\r\n",
        "  cleaned = []\r\n",
        "  for sentence in sentences:\r\n",
        "    cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "    cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "    cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  # get bigrams\r\n",
        "  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, \r\n",
        "                   delimiter=b' ')\r\n",
        "  bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "  bigramed_tokens = []\r\n",
        "  for sent in cleaned:\r\n",
        "      tokens = bigram_phraser[sent]\r\n",
        "      bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # run again to get trigrams\r\n",
        "  trigram = Phrases(bigramed_tokens, min_count=min_count, threshold=threshold, \r\n",
        "                    delimiter=b' ')\r\n",
        "  trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "  trigramed_tokens = []\r\n",
        "  for sent in bigramed_tokens:\r\n",
        "      tokens = trigram_phraser[sent]\r\n",
        "      trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # build a toy model to update with\r\n",
        "  model = Word2Vec(size=300, min_count=5)\r\n",
        "  model.build_vocab(trigramed_tokens)\r\n",
        "  total_examples = model.corpus_count\r\n",
        "\r\n",
        "  # add GloVe's vocabulary & weights\r\n",
        "  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "  # train on our data\r\n",
        "  model.train(trigramed_tokens, total_examples=total_examples, epochs=model.epochs)\r\n",
        "  model_wv = model.wv\r\n",
        "  del model\r\n",
        "  return model_wv"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuWIUSR_Pq2x"
      },
      "source": [
        "pairs_to_try = [(['perception'], []),\r\n",
        "                (['dasein'], []),\r\n",
        "                (['consciousness'], []),\r\n",
        "                (['method'], []),]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIMLCNVHdxID",
        "outputId": "ea8573b6-b5ce-4232-8004-b6f2d649dac0"
      },
      "source": [
        "ph_model = train_glove(school='phenomenology', glove_vectors=glove_vectors)\r\n",
        "\r\n",
        "test_w2v(ph_model, pairs_to_try)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['perception']\tNegative - []\n",
            "- consciousness (0.92737)\n",
            "- care (0.92598)\n",
            "- reality (0.92137)\n",
            "- movement (0.91659)\n",
            "- representation (0.914)\n",
            "\n",
            "Positive - ['dasein']\tNegative - []\n",
            "- being (0.89199)\n",
            "- itself (0.86746)\n",
            "- truth (0.85501)\n",
            "- understanding (0.82309)\n",
            "- everything (0.81982)\n",
            "\n",
            "Positive - ['consciousness']\tNegative - []\n",
            "- perception (0.92737)\n",
            "- care (0.91024)\n",
            "- movement (0.8974)\n",
            "- knowledge (0.8942)\n",
            "- reality (0.88805)\n",
            "\n",
            "Positive - ['method']\tNegative - []\n",
            "- necessity (0.95785)\n",
            "- spirit (0.95634)\n",
            "- origin (0.95564)\n",
            "- historicity (0.95134)\n",
            "- condition (0.95012)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRp6eNmHeP01"
      },
      "source": [
        "Using the phenomenology vectors on some central terms of phenomenology once again yields some pretty compelling results. \r\n",
        "\r\n",
        "These vectors seem to be an effective tool for revealing how a word is used by a school. \r\n",
        "\r\n",
        "As a final kind of exploration of this method, we'll train w2v models in this way for each school and examine how each of them looks a couple of the same words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8_XI24ogh17",
        "outputId": "b78d8168-1691-49c7-c271-1ad43a53ead8"
      },
      "source": [
        "w2v_dict = {}\r\n",
        "\r\n",
        "for school in df['school'].unique():\r\n",
        "  w2v_dict[school] = train_glove(school, glove_vectors=glove_vectors)\r\n",
        "  print(f'{school} completed')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "plato completed\n",
            "aristotle completed\n",
            "empiricism completed\n",
            "rationalism completed\n",
            "analytic completed\n",
            "continental completed\n",
            "phenomenology completed\n",
            "german_idealism completed\n",
            "communism completed\n",
            "capitalism completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZktiqighpT",
        "outputId": "a5b4ceee-734c-4fe8-b2ab-d3ec6b7a5f6b"
      },
      "source": [
        "for school in df['school'].unique():\r\n",
        "  print(f'\\t{school.upper()}')\r\n",
        "  print('----------------------')\r\n",
        "  test_w2v(w2v_dict[school], [(['philosophy'], [])])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPLATO\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- relief (0.94646)\n",
            "- springs (0.94496)\n",
            "- friendship (0.94237)\n",
            "- greece (0.93422)\n",
            "- regime (0.93311)\n",
            "\n",
            "\tARISTOTLE\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- respiration (0.91699)\n",
            "- oratory (0.87146)\n",
            "- shrillness (0.87085)\n",
            "- holders (0.86975)\n",
            "- memory (0.85944)\n",
            "\n",
            "\tEMPIRICISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- religion (0.93495)\n",
            "- mankind (0.92261)\n",
            "- doctrine (0.92259)\n",
            "- inquiry (0.9155)\n",
            "- faith (0.90897)\n",
            "\n",
            "\tRATIONALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- person (0.95782)\n",
            "- return (0.92869)\n",
            "- death (0.92862)\n",
            "- fall (0.92596)\n",
            "- public (0.9258)\n",
            "\n",
            "\tANALYTIC\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- philosophical (0.90815)\n",
            "- hahn (0.85215)\n",
            "- carnap (0.84692)\n",
            "- semantics (0.84195)\n",
            "- davidson (0.83312)\n",
            "\n",
            "\tCONTINENTAL\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- metaphysics (0.9689)\n",
            "- unreason (0.95789)\n",
            "- history (0.95635)\n",
            "- consciousness (0.94364)\n",
            "- proposition (0.93498)\n",
            "\n",
            "\tPHENOMENOLOGY\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- method (0.9159)\n",
            "- metaphysics (0.91034)\n",
            "- spirit (0.90868)\n",
            "- phenomenology (0.89536)\n",
            "- science (0.89134)\n",
            "\n",
            "\tGERMAN_IDEALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- science (0.87517)\n",
            "- metaphysics (0.86891)\n",
            "- method (0.81549)\n",
            "- pure reason (0.78349)\n",
            "- definitions (0.78128)\n",
            "\n",
            "\tCOMMUNISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- ministers (0.99902)\n",
            "- inches (0.999)\n",
            "- syndicates (0.99897)\n",
            "- divisions (0.99894)\n",
            "- along (0.99891)\n",
            "\n",
            "\tCAPITALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- queen (0.99179)\n",
            "- tartar (0.98992)\n",
            "- greek (0.98812)\n",
            "- treaties (0.98806)\n",
            "- uniform (0.98774)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmP9U0WrghgT"
      },
      "source": [
        "Interestingly, many of these top words align quite strongly with the school's general attitude towards philosophy. \r\n",
        "\r\n",
        "The model seems solid - our next step is to train one on the entire corpus for use in classification. We do that, and export it, below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQu70aYwChpm"
      },
      "source": [
        "documents = df['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "all_text_model = Word2Vec(size=300, min_count=5)\r\n",
        "all_text_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = all_text_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "all_text_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "all_text_model.train(trigramed_tokens, total_examples=total_examples, \r\n",
        "                     epochs=all_text_model.epochs)\r\n",
        "all_text_wv = all_text_model.wv\r\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wk3zbVgDbxl"
      },
      "source": [
        "As a test case, let's see how the philosophy thinks of itself as compared to how glove thinks of philosophy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GNhMEqvDnpN",
        "outputId": "b88275e5-5242-40ff-eb0b-379fdcebd3ad"
      },
      "source": [
        "for model in [1, 2]:\r\n",
        "  if model == 1:\r\n",
        "    print(f'\\tPHILOSOPHY CORPUS')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(all_text_wv, [(['philosophy'], [])])\r\n",
        "  if model == 2:\r\n",
        "    print(f'\\tBASE GLOVE')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(glove_vectors, [(['philosophy'], [])])\r\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPHILOSOPHY CORPUS\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- metaphysics (0.78942)\n",
            "- theology (0.76947)\n",
            "- religion (0.73203)\n",
            "- science (0.72824)\n",
            "- philosophical (0.71864)\n",
            "\n",
            "\tBASE GLOVE\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- theology (0.88151)\n",
            "- philosophical (0.84362)\n",
            "- mathematics (0.83389)\n",
            "- psychology (0.82387)\n",
            "- sociology (0.81085)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeVE7mUFzlY"
      },
      "source": [
        "This sort of stands to reason - 'metaphysics' often has a different meaning outside of philosophical discussion, so it's not surprising to see it as the most changed term here. \r\n",
        "\r\n",
        "All in all, things look good, so let's export the vectors so that they can be used in our neural networks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6rnhbD235n"
      },
      "source": [
        "all_text_wv.save('/gdrive/MyDrive/Colab_Projects/Phil_NLP/w2v_models/w2v_for_nn.bin')\r\n",
        "all_text_wv.save('/gdrive/MyDrive/Colab_Projects/Phil_NLP/w2v_models/w2v_for_nn.wordvectors')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nw5Hf0JStaD"
      },
      "source": [
        "for school in w2v_dict.keys():\r\n",
        "  w2v_dict[school].save(f'/gdrive/MyDrive/Colab_Projects/Phil_NLP/w2v_models/{school}_w2v.wordvectors')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wn9YYoGXel"
      },
      "source": [
        "And that's it! See our other notebooks for more of the modeling work. "
      ]
    }
  ]
}