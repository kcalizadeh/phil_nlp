{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fRqjeaLlmZHe",
        "_7J7_WOz2VK8"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNsMMOb1wvsKQHpquGQ5Gc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcalizadeh/phil_nlp/blob/master/w2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxG_6U1lGEa"
      },
      "source": [
        "### Imports and Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRT6BjwbiM8n",
        "outputId": "c78957c4-f147-4484-a54e-d3de16d0620f"
      },
      "source": [
        "# this cell mounts drive, sets the correct directory, then imports all functions\n",
        "# and relevant libraries via the functions.py file\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "# install relevent libraries not included with colab\n",
        "!pip install lime\n",
        "!pip install symspellpy\n",
        "\n",
        "drive.mount('/gdrive',force_remount=True)\n",
        "\n",
        "drive_path = '/gdrive/MyDrive/Colab_Projects/Phil_NLP'\n",
        "\n",
        "sys.path.append(drive_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.6/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (1.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (7.0.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.19.4)\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_FSrsN8l0J4",
        "outputId": "05be46dc-a4c8-4c74-caca-2baf51026398"
      },
      "source": [
        "from functions import *\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random_seed=17"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqjeaLlmZHe"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "PHxy7346l3hZ",
        "outputId": "496b14e5-a342-4f0a-c14c-5709bb88308c"
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Colab_Projects/Phil_NLP/phil_nlp.csv')\n",
        "\n",
        "df.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>school</th>\n",
              "      <th>sentence_spacy</th>\n",
              "      <th>sentence_str</th>\n",
              "      <th>sentence_length</th>\n",
              "      <th>sentence_lowered</th>\n",
              "      <th>lemmatized_str</th>\n",
              "      <th>tokenized_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5112</th>\n",
              "      <td>complete works</td>\n",
              "      <td>Plato</td>\n",
              "      <td>plato</td>\n",
              "      <td>But when he has also got an account of it, he ...</td>\n",
              "      <td>But when he has also got an account of it, he ...</td>\n",
              "      <td>102</td>\n",
              "      <td>but when he has also got an account of it, he ...</td>\n",
              "      <td>but when -PRON- have also get an account of -...</td>\n",
              "      <td>['But', 'when', 'he', 'has', 'also', 'got', 'a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287030</th>\n",
              "      <td>elements of right</td>\n",
              "      <td>Hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>In Plato's republic, subjective freedom is not...</td>\n",
              "      <td>In Plato's republic, subjective freedom is not...</td>\n",
              "      <td>154</td>\n",
              "      <td>in plato's republic, subjective freedom is not...</td>\n",
              "      <td>in Plato 's republic , subjective freedom be ...</td>\n",
              "      <td>['In', 'Plato', \"'s\", 'republic', ',', 'subjec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159907</th>\n",
              "      <td>quintessence</td>\n",
              "      <td>Quine</td>\n",
              "      <td>analytic</td>\n",
              "      <td>This becomes clear as soon as Vl a rephrased i...</td>\n",
              "      <td>This becomes clear as soon as Vl a rephrased i...</td>\n",
              "      <td>91</td>\n",
              "      <td>this becomes clear as soon as vl a rephrased i...</td>\n",
              "      <td>this become clear as soon as Vl a rephrase in...</td>\n",
              "      <td>['This', 'becomes', 'clear', 'as', 'soon', 'as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4416</th>\n",
              "      <td>complete works</td>\n",
              "      <td>Plato</td>\n",
              "      <td>plato</td>\n",
              "      <td>When his companions become lyric on the subjec...</td>\n",
              "      <td>When his companions become lyric on the subjec...</td>\n",
              "      <td>502</td>\n",
              "      <td>when his companions become lyric on the subjec...</td>\n",
              "      <td>when -PRON- companion become lyric on the sub...</td>\n",
              "      <td>['When', 'his', 'companions', 'become', 'lyric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28437</th>\n",
              "      <td>complete works</td>\n",
              "      <td>Plato</td>\n",
              "      <td>plato</td>\n",
              "      <td>So if I to tell the story of how it really cam...</td>\n",
              "      <td>So if I to tell the story of how it really cam...</td>\n",
              "      <td>170</td>\n",
              "      <td>so if i to tell the story of how it really cam...</td>\n",
              "      <td>so if -PRON- to tell the story of how -PRON- ...</td>\n",
              "      <td>['So', 'if', 'I', 'to', 'tell', 'the', 'story'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    title  ...                                      tokenized_txt\n",
              "5112       complete works  ...  ['But', 'when', 'he', 'has', 'also', 'got', 'a...\n",
              "287030  elements of right  ...  ['In', 'Plato', \"'s\", 'republic', ',', 'subjec...\n",
              "159907       quintessence  ...  ['This', 'becomes', 'clear', 'as', 'soon', 'as...\n",
              "4416       complete works  ...  ['When', 'his', 'companions', 'become', 'lyric...\n",
              "28437      complete works  ...  ['So', 'if', 'I', 'to', 'tell', 'the', 'story'...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR-d0hRI7hmQ"
      },
      "source": [
        "# using gensim's built-in tokenizer \r\n",
        "df['gensim_tokenized'] = df['sentence_str'].map(lambda x: simple_preprocess(x.lower(),deacc=True,\r\n",
        "                                                        max_len=100))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-mTBCSDojS1",
        "outputId": "c5c4fd4c-80fd-4c61-9e79-4e66fe0816cb"
      },
      "source": [
        "# check how it worked\r\n",
        "print(df.iloc[216282]['sentence_str'])\r\n",
        "df['gensim_tokenized'][216282]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The bumble bee is a part of the reproductive system of the clover.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'bumble',\n",
              " 'bee',\n",
              " 'is',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'reproductive',\n",
              " 'system',\n",
              " 'of',\n",
              " 'the',\n",
              " 'clover']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KxV2JTIoyxE"
      },
      "source": [
        "Well with that beautiful little quote, we are ready to start training our w2v model! At first we'll focus on a single school, since a single school is more likely to have consistency in their use of a word.\r\n",
        "\r\n",
        "Unfortunately, we didn't have much luck with just training on the texts alone. The code for it is left here for posterity, but it was when we worked with GloVe as the base that we had results that were actually useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th5FA84Ip-ha"
      },
      "source": [
        "### Word 2 Vec Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cv5UgQEqEGK"
      },
      "source": [
        "#### German Idealism as a Test Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFuFkbN7qN2w"
      },
      "source": [
        "We start by examining the texts of German Idealism to get a feel for what kind of parameters would work best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78GJHWsMF2he"
      },
      "source": [
        "def make_w2v(series, stopwords=[], size=200, window=5, min_count=5, workers=-1, \r\n",
        "             epochs=20, lowercase=True, sg=0, seed=17, cbow_mean=1, alpha=0.025,\r\n",
        "             sample=0.001, use_bigrams=True, threshold=10, bigram_min=5):\r\n",
        "  # turn the series into a list, lower it, clean it\r\n",
        "    sentences = [sentence for sentence in series]\r\n",
        "    if lowercase:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "    else:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  # incorporate bigrams\r\n",
        "    if use_bigrams:\r\n",
        "      bigram = Phrases(cleaned, min_count=bigram_min, threshold=threshold, delimiter=b' ')\r\n",
        "      bigram_phraser = Phraser(bigram)\r\n",
        "      tokens_list = []\r\n",
        "      for sent in cleaned:\r\n",
        "        tokens_ = bigram_phraser[sent]\r\n",
        "        tokens_list.append(tokens_)\r\n",
        "      cleaned = tokens_list\r\n",
        "    else:\r\n",
        "      cleaned = cleaned\r\n",
        "\r\n",
        "  # build the model\r\n",
        "    model = Word2Vec(cleaned, size=size, window=window, \r\n",
        "                     min_count=min_count, workers=workers, seed=seed, sg=sg,\r\n",
        "                     cbow_mean=cbow_mean, alpha=alpha, sample=sample)\r\n",
        "    model.train(series, total_examples=model.corpus_count, epochs=epochs)\r\n",
        "    model_wv = model.wv\r\n",
        "    \r\n",
        "  # clear it to avoid unwanted transference\r\n",
        "    del model\r\n",
        "\r\n",
        "    return model_wv"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQLm1mhOO6w"
      },
      "source": [
        "gi_wv = make_w2v(df[df['school'] == 'german_idealism']['gensim_tokenized'], threshold=12)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUf7kAzvrh7K"
      },
      "source": [
        "We can check this model by trying out a few words. For that purpose we have a testing function that tries some common word combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMJat1b1rhwK"
      },
      "source": [
        "pairs_to_try = [(['law', 'moral'], []),\r\n",
        "                (['self', 'consciousness'], []),\r\n",
        "                (['dialectic'], []),\r\n",
        "                (['logic'], []),\r\n",
        "]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXOnjOLpOcxi",
        "outputId": "bcd7b129-99c4-4689-f8e4-3f738bedace5"
      },
      "source": [
        "test_w2v_pos_neg(gi_wv, pairs_to_try)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- how (0.23285)\n",
            "- inside (0.22697)\n",
            "- original source (0.22589)\n",
            "- acid (0.21729)\n",
            "- df (0.21704)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- discovered (0.26501)\n",
            "- gans (0.24422)\n",
            "- upper hand (0.23032)\n",
            "- blue (0.22977)\n",
            "- experi (0.22654)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- accorded (0.27038)\n",
            "- introduced (0.2446)\n",
            "- ance (0.23565)\n",
            "- quadratic (0.23303)\n",
            "- sublating (0.23201)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- interpenetration (0.27604)\n",
            "- negating (0.25452)\n",
            "- mind (0.24488)\n",
            "- quired (0.24322)\n",
            "- defining (0.2347)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgu0ee-frhZQ"
      },
      "source": [
        "Although some of these make a modicum of sense a lot of them seem like just gibberish. Let's try messing with some parameters.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS8zfzZHrhNe"
      },
      "source": [
        "##### Trying Skip-gram instead of C-bow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiV-E9xzrhCL"
      },
      "source": [
        "# make a base model with the preset parameters\r\n",
        "skip_gi_wv = make_w2v(series = df[df['school'] == 'german_idealism']['gensim_tokenized'], \r\n",
        "                         stopwords=[], sg=1, seed=0)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS_Jzbgirg1y",
        "outputId": "4c9c80ba-98b7-409c-ba73-033fe3640a16"
      },
      "source": [
        "test_w2v(skip_gi_wv, pairs_to_try)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- philosophical works (0.26632)\n",
            "- mathematical (0.25058)\n",
            "- sameness (0.24794)\n",
            "- ens rationis (0.24681)\n",
            "- suggests (0.23576)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- calls (0.25984)\n",
            "- elegance (0.24366)\n",
            "- acquisition (0.23788)\n",
            "- sine (0.232)\n",
            "- final purpose (0.23005)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- transcendental doctrine (0.27465)\n",
            "- middle terms (0.26202)\n",
            "- understood (0.25658)\n",
            "- applied logic (0.25506)\n",
            "- clings (0.22976)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- deciding (0.30243)\n",
            "- rec (0.28958)\n",
            "- possible experience (0.25629)\n",
            "- calculating (0.25121)\n",
            "- mathematical (0.23836)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnNnqhueyaK-"
      },
      "source": [
        "These seem mildy more sensible. Let's tweak the other parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAv1PjLD7ZSh"
      },
      "source": [
        "##### Parameter Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Y3AggQyaVD",
        "outputId": "f110fe50-2ed9-48cf-93a0-6158f5202cb3"
      },
      "source": [
        "model_v1 = make_w2v(df[df['school'] == 'german_idealism']['gensim_tokenized'],\r\n",
        "                       stopwords=[],\r\n",
        "                       size=500,\r\n",
        "                       window=5,\r\n",
        "                       min_count=25,\r\n",
        "                       epochs=10,\r\n",
        "                       sg=1, \r\n",
        "                       seed=45)\r\n",
        "\r\n",
        "len(model_v1.vocab)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVVqIAf8yadZ",
        "outputId": "3ded5504-d31e-4693-bde5-8b6342cd6cb1"
      },
      "source": [
        "test_w2v(model_v1, pairs_to_try)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- consistent (0.16991)\n",
            "- unless (0.14104)\n",
            "- distinguish (0.13626)\n",
            "- rose (0.13344)\n",
            "- she (0.1314)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- performed (0.15893)\n",
            "- itselfness (0.15161)\n",
            "- impossibility (0.14681)\n",
            "- their (0.14224)\n",
            "- more closely (0.13561)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- empty space (0.17288)\n",
            "- manifested (0.15537)\n",
            "- remark (0.15018)\n",
            "- associated with (0.14597)\n",
            "- distance (0.13942)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- premise (0.15924)\n",
            "- test (0.15442)\n",
            "- cognitions (0.1468)\n",
            "- slightest (0.13728)\n",
            "- notion (0.13671)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn8zvu2z2U_o"
      },
      "source": [
        "Despite tweaking parameters far and wide, it's difficult to get any results that are compellingly sensible. In most cases there are one or two terms in the similarity list that make some sense but others that are just strange or unconnected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7J7_WOz2VK8"
      },
      "source": [
        "#### Trying Another School"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZdUtK642VcE",
        "outputId": "9f71b473-5a6a-4ffb-95c3-1fffe2bfde38"
      },
      "source": [
        "cm_w2v = make_w2v(df[df['school'] == 'communism']['gensim_tokenized'],\r\n",
        "                       stopwords=[],\r\n",
        "                       size=700,\r\n",
        "                       window=10,\r\n",
        "                       min_count=10,\r\n",
        "                       epochs=25,\r\n",
        "                       sg=1, \r\n",
        "                       seed=10)\r\n",
        "\r\n",
        "type(cm_w2v)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gensim.models.keyedvectors.Word2VecKeyedVectors"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYoO0ozP2VmK"
      },
      "source": [
        "pairs_to_try=[(['material', 'conditions'], []),\r\n",
        "              (['worker'], ['owner']),\r\n",
        "              (['alienation', 'labor'], []),\r\n",
        "              (['capital'], [])]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1809IvVx2Vuy",
        "outputId": "fa6f7977-d1b6-4595-c793-4a8f533888d0"
      },
      "source": [
        "test_w2v(cm_w2v, pairs_to_try)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['material', 'conditions']\tNegative - []\n",
            "- machines (0.13411)\n",
            "- at night (0.12783)\n",
            "- renewal (0.12365)\n",
            "- paupers (0.12364)\n",
            "- current (0.11691)\n",
            "\n",
            "Positive - ['worker']\tNegative - ['owner']\n",
            "- taken place (0.12344)\n",
            "- peasantry (0.12229)\n",
            "- public (0.12178)\n",
            "- socialists (0.11708)\n",
            "- governments (0.1158)\n",
            "\n",
            "Positive - ['alienation', 'labor']\tNegative - []\n",
            "- subsistence (0.137)\n",
            "- occupy (0.12043)\n",
            "- people (0.11918)\n",
            "- pole (0.11914)\n",
            "- determine (0.1153)\n",
            "\n",
            "Positive - ['capital']\tNegative - []\n",
            "- actively (0.15668)\n",
            "- consisting (0.12746)\n",
            "- component (0.12398)\n",
            "- issued (0.11602)\n",
            "- growth (0.11515)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hUYWlYr2V3L"
      },
      "source": [
        "Here the results were similar - a few words that made some sense and plenty that were just odd.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5ZnODM2V_P"
      },
      "source": [
        "### Transfer Learning with GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAOaelq2WIr"
      },
      "source": [
        "We'll import GloVe vectors as w2v, then use those as a base from which to train new vectors that are tuned to our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcFZMRT82WRC"
      },
      "source": [
        "glove_file = datapath('/gdrive/MyDrive/Colab_Projects/Phil_NLP/glove.6B.50d.txt')\r\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\r\n",
        "\r\n",
        "_ = glove2word2vec(glove_file, tmp_file)\r\n",
        "\r\n",
        "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjdDWtd72Tli",
        "outputId": "6ad84275-19ce-4ee2-871a-654f8ddbb72b"
      },
      "source": [
        "# check out how GloVe works on our test pairs\r\n",
        "test_w2v_pos_neg(glove_vectors, pairs_to_try)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- morality (0.82654)\n",
            "- legal (0.82652)\n",
            "- laws (0.81529)\n",
            "- constitutional (0.80616)\n",
            "- fundamental (0.80217)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- sense (0.83446)\n",
            "- mind (0.79755)\n",
            "- vision (0.78202)\n",
            "- belief (0.78031)\n",
            "- life (0.77984)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- hegelian (0.88376)\n",
            "- dialectical (0.83417)\n",
            "- dialectics (0.80672)\n",
            "- materialist (0.77674)\n",
            "- metaphysics (0.77488)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- reasoning (0.81405)\n",
            "- intuitionistic (0.76531)\n",
            "- concepts (0.75831)\n",
            "- logical (0.75604)\n",
            "- theory (0.75026)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFyauZoo7KW"
      },
      "source": [
        "Ok these make a lot more sense right from the start. But we want them to be trained on our actual philosophical texts - that way we can see how different thinkers use different words and potentially use the vectors for classification.\r\n",
        "\r\n",
        "So in the cells below we train the existing GloVe model on on the German Idealist texts as a test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSOA8Zz2o7vX"
      },
      "source": [
        "# isolate the relevant school\r\n",
        "documents = df[df['school'] == 'german_idealism']['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "base_model = Word2Vec(size=300, min_count=5)\r\n",
        "base_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = base_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "base_model.train(trigramed_tokens, total_examples=total_examples, epochs=base_model.epochs)\r\n",
        "base_model_wv = base_model.wv\r\n",
        "del base_model"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0FjXahncQya",
        "outputId": "c6e92eb8-867a-4cfb-c855-c9d4bf1f6253"
      },
      "source": [
        "test_w2v(base_model_wv, pairs_to_try)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- moral law (0.80435)\n",
            "- freedom (0.79501)\n",
            "- rule (0.79358)\n",
            "- morality (0.78824)\n",
            "- highest good (0.78263)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- self consciousness (0.9147)\n",
            "- externality (0.87614)\n",
            "- immediacy (0.8751)\n",
            "- essence (0.87467)\n",
            "- negativity (0.86413)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- doctrine (0.91102)\n",
            "- antinomy (0.90098)\n",
            "- exposition (0.89884)\n",
            "- method (0.8963)\n",
            "- deduction (0.89234)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- pure reason (0.85686)\n",
            "- doctrine (0.84364)\n",
            "- method (0.83159)\n",
            "- idealism (0.81921)\n",
            "- metaphysics (0.81001)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3MnHFbcl7d"
      },
      "source": [
        "We can immediately see that these make a lot more sense (and the similarity scores are a lot higher). Self-consciousness is commonly associated with freedom in German idealism, logic with metaphysics, and the moral law with universality and the good. This is a massive improvement - these vectors can be fairly said to reflect how german idealists use these terms. Moreover, they are significantly different than the original GloVe model, which indicates that there was real learning going on here.\r\n",
        "\r\n",
        "For comparison, let's check these same terms, but as used by Phenomenologists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1Jkkc7dqk9"
      },
      "source": [
        "def train_glove(school, glove_vectors, threshold=10, stopwords=[],\r\n",
        "                min_count=20):\r\n",
        "  # isolate the relevant school\r\n",
        "  documents = df[df['school'] ==school]['gensim_tokenized']\r\n",
        "\r\n",
        "  # format the series to be used\r\n",
        "  stopwords = []\r\n",
        "\r\n",
        "  sentences = [sentence for sentence in documents]\r\n",
        "  cleaned = []\r\n",
        "  for sentence in sentences:\r\n",
        "    cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "    cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "    cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  # get bigrams\r\n",
        "  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, \r\n",
        "                   delimiter=b' ')\r\n",
        "  bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "  bigramed_tokens = []\r\n",
        "  for sent in cleaned:\r\n",
        "      tokens = bigram_phraser[sent]\r\n",
        "      bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # run again to get trigrams\r\n",
        "  trigram = Phrases(bigramed_tokens, min_count=min_count, threshold=threshold, \r\n",
        "                    delimiter=b' ')\r\n",
        "  trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "  trigramed_tokens = []\r\n",
        "  for sent in bigramed_tokens:\r\n",
        "      tokens = trigram_phraser[sent]\r\n",
        "      trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # build a toy model to update with\r\n",
        "  model = Word2Vec(size=300, min_count=5)\r\n",
        "  model.build_vocab(trigramed_tokens)\r\n",
        "  total_examples = model.corpus_count\r\n",
        "\r\n",
        "  # add GloVe's vocabulary & weights\r\n",
        "  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "  # train on our data\r\n",
        "  model.train(trigramed_tokens, total_examples=total_examples, epochs=model.epochs)\r\n",
        "  model_wv = model.wv\r\n",
        "  del model\r\n",
        "  return model_wv"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIMLCNVHdxID",
        "outputId": "9aaf3d7d-3a0f-466b-fbe1-803d267ba8fb"
      },
      "source": [
        "ph_model = train_glove(school='phenomenology', glove_vectors=glove_vectors)\r\n",
        "\r\n",
        "test_w2v(ph_model, pairs_to_try)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- linguistic (0.99409)\n",
            "- categorial (0.99366)\n",
            "- defining (0.99345)\n",
            "- chain (0.99338)\n",
            "- clarifying (0.9928)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- existence (0.95972)\n",
            "- nature (0.95558)\n",
            "- potentiality (0.95472)\n",
            "- authentic (0.94812)\n",
            "- understanding (0.94194)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- formerly (0.99032)\n",
            "- confusion (0.99006)\n",
            "- consideration (0.98976)\n",
            "- dialogue (0.98914)\n",
            "- reorientation (0.98834)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- transformation (0.98935)\n",
            "- definition (0.98631)\n",
            "- explication (0.9863)\n",
            "- ity (0.98405)\n",
            "- articulation (0.98383)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRp6eNmHeP01"
      },
      "source": [
        "These are substantially different - which is good since the schools are substantially different. They mostly make sense, except for words like 'dialectic' which are rarely used by phenomenologists. The general attitude towards logic as a cold sterilizing force is evident, as is their emphasis on perception and natural life, as compared to the German idealist emphasis on abstractions like universality and freedom (see 'self consciousness'). \r\n",
        "\r\n",
        "These vectors seem to be an effective tool for revealing word usage between the schools. \r\n",
        "\r\n",
        "As a final kind of exploration of this method, we'll train w2v models in this way for each school and examine how each of them looks a couple of the same words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8_XI24ogh17",
        "outputId": "806544d6-7e22-42d6-c470-7035b4705f64"
      },
      "source": [
        "w2v_dict = {}\r\n",
        "\r\n",
        "for school in df['school'].unique():\r\n",
        "  w2v_dict[school] = train_glove(school, glove_vectors=glove_vectors)\r\n",
        "  print(f'{school} completed')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "plato completed\n",
            "aristotle completed\n",
            "empiricism completed\n",
            "rationalism completed\n",
            "analytic completed\n",
            "continental completed\n",
            "phenomenology completed\n",
            "german_idealism completed\n",
            "communism completed\n",
            "capitalism completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZktiqighpT",
        "outputId": "6988b96d-f173-43e0-8410-1d66a6b48639"
      },
      "source": [
        "for school in df['school'].unique():\r\n",
        "  print(f'\\t{school.upper()}')\r\n",
        "  print('----------------------')\r\n",
        "  test_w2v(w2v_dict[school], [(['philosophy'], [])])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPLATO\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- relief (0.95119)\n",
            "- parentage (0.94101)\n",
            "- flesh (0.94014)\n",
            "- caution (0.9373)\n",
            "- youth (0.93688)\n",
            "\n",
            "\tARISTOTLE\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- cretan (0.86985)\n",
            "- student (0.86937)\n",
            "- practice (0.86075)\n",
            "- studied (0.85763)\n",
            "- economy (0.85579)\n",
            "\n",
            "\tEMPIRICISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- practice (0.95716)\n",
            "- religion (0.95474)\n",
            "- history (0.94577)\n",
            "- inquiry (0.94528)\n",
            "- vulgar (0.93813)\n",
            "\n",
            "\tRATIONALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- church (0.95102)\n",
            "- passage (0.94926)\n",
            "- beginning (0.94339)\n",
            "- verses (0.93945)\n",
            "- treatise (0.93924)\n",
            "\n",
            "\tANALYTIC\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- semantics (0.88169)\n",
            "- philosophical (0.86495)\n",
            "- scope (0.85655)\n",
            "- science (0.85403)\n",
            "- inductive logic (0.84086)\n",
            "\n",
            "\tCONTINENTAL\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- history (0.96353)\n",
            "- speech (0.94236)\n",
            "- gaze (0.94186)\n",
            "- metaphysics (0.9379)\n",
            "- notion (0.93487)\n",
            "\n",
            "\tPHENOMENOLOGY\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- metaphysics (0.91094)\n",
            "- extremity (0.89628)\n",
            "- psychology (0.89514)\n",
            "- spirit (0.89398)\n",
            "- history (0.89031)\n",
            "\n",
            "\tGERMAN_IDEALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- science (0.89329)\n",
            "- metaphysics (0.8762)\n",
            "- theory (0.83056)\n",
            "- pure reason (0.82991)\n",
            "- critique (0.82398)\n",
            "\n",
            "\tCOMMUNISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- european (0.99892)\n",
            "- accounts (0.99887)\n",
            "- bankers (0.99886)\n",
            "- breaks (0.99883)\n",
            "- manufactory (0.9988)\n",
            "\n",
            "\tCAPITALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- faction (0.99004)\n",
            "- anciently (0.98945)\n",
            "- spirits (0.98908)\n",
            "- apprenticeships (0.98822)\n",
            "- re (0.98746)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmP9U0WrghgT"
      },
      "source": [
        "Interestingly, many of these top words align quite strongly with the school's general attitude towards philosophy. \r\n",
        "\r\n",
        "The model seems solid - our next step is to train one on the entire corpus for use in classification. We do that, and export it, below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQu70aYwChpm"
      },
      "source": [
        "documents = df['gensim_tokenized']\r\n",
        "\r\n",
        "# format the series to be used\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "# get bigrams\r\n",
        "bigram = Phrases(cleaned, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "bigramed_tokens = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens = bigram_phraser[sent]\r\n",
        "    bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# run again to get trigrams\r\n",
        "trigram = Phrases(bigramed_tokens, min_count=30, threshold=10, \r\n",
        "                  delimiter=b' ')\r\n",
        "trigram_phraser = Phraser(trigram)\r\n",
        "\r\n",
        "trigramed_tokens = []\r\n",
        "for sent in bigramed_tokens:\r\n",
        "    tokens = trigram_phraser[sent]\r\n",
        "    trigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "all_text_model = Word2Vec(size=300, min_count=5)\r\n",
        "all_text_model.build_vocab(trigramed_tokens)\r\n",
        "total_examples = all_text_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "all_text_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "all_text_model.train(trigramed_tokens, total_examples=total_examples, \r\n",
        "                     epochs=all_text_model.epochs)\r\n",
        "all_text_wv = all_text_model.wv\r\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wk3zbVgDbxl"
      },
      "source": [
        "As a test case, let's see how the philosophy thinks of itself as compared to how glove thinks of philosophy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GNhMEqvDnpN",
        "outputId": "eb729a1b-7722-48a0-cf4f-3949a56c1323"
      },
      "source": [
        "for model in [1, 2]:\r\n",
        "  if model == 1:\r\n",
        "    print(f'\\tPHILOSOPHY CORPUS')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(all_text_wv, [(['philosophy'], [])])\r\n",
        "  if model == 2:\r\n",
        "    print(f'\\tBASE GLOVE')\r\n",
        "    print('------------------------------------')\r\n",
        "    test_w2v(glove_vectors, [(['philosophy'], [])])\r\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPHILOSOPHY CORPUS\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- metaphysics (0.79279)\n",
            "- theology (0.77741)\n",
            "- science (0.7549)\n",
            "- philosophical (0.73011)\n",
            "- psychology (0.72825)\n",
            "\n",
            "\tBASE GLOVE\n",
            "------------------------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- theology (0.88151)\n",
            "- philosophical (0.84362)\n",
            "- mathematics (0.83389)\n",
            "- psychology (0.82387)\n",
            "- sociology (0.81085)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeVE7mUFzlY"
      },
      "source": [
        "This sort of stands to reason - 'metaphysics' often has a different meaning outside of philosophical discussion, so it's not surprising to see it as the most changed term here. \r\n",
        "\r\n",
        "All in all, things look good, so let's export the vectors so that they can be used in our neural networks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6rnhbD235n"
      },
      "source": [
        "all_text_wv.save_word2vec_format('/gdrive/MyDrive/Colab_Projects/Phil_NLP/w2v_for_nn.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wn9YYoGXel"
      },
      "source": [
        "And that's it! See our other notebooks for more of the modeling work. "
      ]
    }
  ]
}