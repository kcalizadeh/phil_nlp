{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fRqjeaLlmZHe",
        "_7J7_WOz2VK8"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO4Rlt5LBe41WePB8BkiVYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcalizadeh/phil_nlp/blob/master/w2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrxG_6U1lGEa"
      },
      "source": [
        "### Imports and Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRT6BjwbiM8n",
        "outputId": "466c4721-f042-4a71-e504-39dbb29c82cb"
      },
      "source": [
        "# this cell mounts drive, sets the correct directory, then imports all functions\n",
        "# and relevant libraries via the functions.py file\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "# install relevent libraries not included with colab\n",
        "!pip install lime\n",
        "!pip install symspellpy\n",
        "\n",
        "drive.mount('/gdrive',force_remount=True)\n",
        "\n",
        "drive_path = '/gdrive/MyDrive/Colab_Projects/Phil_NLP'\n",
        "\n",
        "sys.path.append(drive_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/86/91a13127d83d793ecb50eb75e716f76e6eda809b6803c5a4ff462339789e/lime-0.2.0.1.tar.gz (275kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 24.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 18.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 11.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 12.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 12.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 12.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 12.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (1.0.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-cp36-none-any.whl size=283846 sha256=3d32d4a7ab445133c5b13d801f11a69a063b967b35c9f71327c5b8acbdacabc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/4f/a5/0bc765457bd41378bf3ce8d17d7495369d6e7ca3b712c60c89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n",
            "Collecting symspellpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/af/e71fcca6a42b6a63f518b0c1627e1f67822815cb0cf71e6af05acbd75c78/symspellpy-6.7.0-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.19.4)\n",
            "Installing collected packages: symspellpy\n",
            "Successfully installed symspellpy-6.7.0\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_FSrsN8l0J4",
        "outputId": "49683903-7c43-4311-c089-16ed9c0e2d94"
      },
      "source": [
        "from functions import *\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random_seed=17"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRqjeaLlmZHe"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "PHxy7346l3hZ",
        "outputId": "e8648a1e-4fae-40ed-e69d-6b2fdaf64c56"
      },
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Colab_Projects/Phil_NLP/phil_nlp.csv')\n",
        "\n",
        "df.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>school</th>\n",
              "      <th>sentence_spacy</th>\n",
              "      <th>sentence_str</th>\n",
              "      <th>sentence_length</th>\n",
              "      <th>sentence_lowered</th>\n",
              "      <th>lemmatized_str</th>\n",
              "      <th>tokenized_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>291113</th>\n",
              "      <td>kapital</td>\n",
              "      <td>Marx</td>\n",
              "      <td>communism</td>\n",
              "      <td>They count merely as depositories of so much a...</td>\n",
              "      <td>They count merely as depositories of so much a...</td>\n",
              "      <td>349</td>\n",
              "      <td>they count merely as depositories of so much a...</td>\n",
              "      <td>-PRON- count merely as depository of so much ...</td>\n",
              "      <td>['They', 'count', 'merely', 'as', 'depositorie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105922</th>\n",
              "      <td>three dialogues</td>\n",
              "      <td>Berkeley</td>\n",
              "      <td>empiricism</td>\n",
              "      <td>But here lies the difference.</td>\n",
              "      <td>But here lies the difference.</td>\n",
              "      <td>29</td>\n",
              "      <td>but here lies the difference.</td>\n",
              "      <td>but here lie the difference .</td>\n",
              "      <td>['But', 'here', 'lies', 'the', 'difference', '.']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139680</th>\n",
              "      <td>philosophical investigations</td>\n",
              "      <td>Wittgenstein</td>\n",
              "      <td>analytic</td>\n",
              "      <td>Let us first discuss this point of the argumen...</td>\n",
              "      <td>Let us first discuss this point of the argumen...</td>\n",
              "      <td>105</td>\n",
              "      <td>let us first discuss this point of the argumen...</td>\n",
              "      <td>let -PRON- first discuss this point of the ar...</td>\n",
              "      <td>['Let', 'us', 'first', 'discuss', 'this', 'poi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57671</th>\n",
              "      <td>complete works vol 1</td>\n",
              "      <td>Aristotle</td>\n",
              "      <td>aristotle</td>\n",
              "      <td>If it thunders while a hen bird is brooding, t...</td>\n",
              "      <td>If it thunders while a hen bird is brooding, t...</td>\n",
              "      <td>65</td>\n",
              "      <td>if it thunders while a hen bird is brooding, t...</td>\n",
              "      <td>if -PRON- thunder while a hen bird be brood ,...</td>\n",
              "      <td>['If', 'it', 'thunders', 'while', 'a', 'hen', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179999</th>\n",
              "      <td>philosophical troubles</td>\n",
              "      <td>Kripke</td>\n",
              "      <td>analytic</td>\n",
              "      <td>Quine Two Dogmas of Empiricism is no doubt the...</td>\n",
              "      <td>Quine Two Dogmas of Empiricism is no doubt the...</td>\n",
              "      <td>92</td>\n",
              "      <td>quine two dogmas of empiricism is no doubt the...</td>\n",
              "      <td>Quine two Dogmas of empiricism be no doubt th...</td>\n",
              "      <td>['Quine', 'Two', 'Dogmas', 'of', 'Empiricism',...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               title  ...                                      tokenized_txt\n",
              "291113                       kapital  ...  ['They', 'count', 'merely', 'as', 'depositorie...\n",
              "105922               three dialogues  ...  ['But', 'here', 'lies', 'the', 'difference', '.']\n",
              "139680  philosophical investigations  ...  ['Let', 'us', 'first', 'discuss', 'this', 'poi...\n",
              "57671           complete works vol 1  ...  ['If', 'it', 'thunders', 'while', 'a', 'hen', ...\n",
              "179999        philosophical troubles  ...  ['Quine', 'Two', 'Dogmas', 'of', 'Empiricism',...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR-d0hRI7hmQ"
      },
      "source": [
        "# using gensim's built-in tokenizer \r\n",
        "df['gensim_tokenized'] = df['sentence_str'].map(lambda x: simple_preprocess(x.lower(),deacc=True,\r\n",
        "                                                        max_len=100))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-mTBCSDojS1",
        "outputId": "2bdb7118-c264-4228-a11f-bd766ae70aa6"
      },
      "source": [
        "# check how it worked\r\n",
        "print(df.iloc[216282]['sentence_str'])\r\n",
        "df['gensim_tokenized'][216282]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The bumble bee is a part of the reproductive system of the clover.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'bumble',\n",
              " 'bee',\n",
              " 'is',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'reproductive',\n",
              " 'system',\n",
              " 'of',\n",
              " 'the',\n",
              " 'clover']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KxV2JTIoyxE"
      },
      "source": [
        "Well with that beautiful little quote, we are ready to start training our w2v model! At first we'll focus on a single school, since a single school is more likely to have consistency in their use of a word.\r\n",
        "\r\n",
        "Unfortunately, we didn't have much luck with just training on the texts alone. The code for it is left here for posterity, but it was when we worked with GloVe as the base that we had results that were actually useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th5FA84Ip-ha"
      },
      "source": [
        "### Word 2 Vec Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cv5UgQEqEGK"
      },
      "source": [
        "#### German Idealism as a Test Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFuFkbN7qN2w"
      },
      "source": [
        "We start by examining the texts of German Idealism to get a feel for what kind of parameters would work best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78GJHWsMF2he"
      },
      "source": [
        "def make_w2v(series, stopwords=[], size=200, window=5, min_count=5, workers=-1, \r\n",
        "             epochs=20, lowercase=True, sg=0, seed=17, cbow_mean=1, alpha=0.025,\r\n",
        "             sample=0.001, use_bigrams=True, threshold=10, bigram_min=5):\r\n",
        "  # turn the series into a list, lower it, clean it\r\n",
        "    sentences = [sentence for sentence in series]\r\n",
        "    if lowercase:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "    else:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  # incorporate bigrams\r\n",
        "    if use_bigrams:\r\n",
        "      bigram = Phrases(cleaned, min_count=bigram_min, threshold=threshold, delimiter=b' ')\r\n",
        "      bigram_phraser = Phraser(bigram)\r\n",
        "      tokens_list = []\r\n",
        "      for sent in cleaned:\r\n",
        "        tokens_ = bigram_phraser[sent]\r\n",
        "        tokens_list.append(tokens_)\r\n",
        "      cleaned = tokens_list\r\n",
        "    else:\r\n",
        "      cleaned = cleaned\r\n",
        "\r\n",
        "  # build the model\r\n",
        "    model = Word2Vec(cleaned, size=size, window=window, \r\n",
        "                     min_count=min_count, workers=workers, seed=seed, sg=sg,\r\n",
        "                     cbow_mean=cbow_mean, alpha=alpha, sample=sample)\r\n",
        "    model.train(series, total_examples=model.corpus_count, epochs=epochs)\r\n",
        "    model_wv = model.wv\r\n",
        "    \r\n",
        "  # clear it to avoid unwanted transference\r\n",
        "    del model\r\n",
        "\r\n",
        "    return model_wv"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQLm1mhOO6w"
      },
      "source": [
        "bigram_gi = make_w2v(df[df['school'] == 'german_idealism']['gensim_tokenized'], threshold=12)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUf7kAzvrh7K"
      },
      "source": [
        "We can check this model by trying out a few words. For that purpose we have a testing function that tries some common word combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMJat1b1rhwK"
      },
      "source": [
        "pairs_to_try = [(['law', 'moral'], []),\r\n",
        "                (['self', 'consciousness'], []),\r\n",
        "                (['dialectic'], []),\r\n",
        "                (['logic'], []),\r\n",
        "]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXOnjOLpOcxi",
        "outputId": "939d33d0-e451-42dd-a4d1-37ab0ddae12a"
      },
      "source": [
        "test_w2v_pos_neg(bigram_gi, pairs_to_try)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- briefly (0.22728)\n",
            "- de (0.226)\n",
            "- affect (0.21829)\n",
            "- causal connection (0.21455)\n",
            "- fanaticism (0.21406)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- philosophers (0.22425)\n",
            "- born (0.21982)\n",
            "- been able (0.21365)\n",
            "- norm (0.21355)\n",
            "- transformation (0.21213)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- instituted (0.28496)\n",
            "- deputies (0.26338)\n",
            "- alter (0.25104)\n",
            "- energy (0.25003)\n",
            "- illustration (0.22995)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- agreeing (0.24091)\n",
            "- postulated (0.23968)\n",
            "- quite different (0.23708)\n",
            "- organized beings (0.2285)\n",
            "- superficiality (0.2269)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgu0ee-frhZQ"
      },
      "source": [
        "Although some of these make a modicum of sense a lot of them seem like just gibberish. Let's try messing with some parameters.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oN1gBWjOi6F",
        "outputId": "59ed91d6-5520-4403-80aa-b5a10de3fd76"
      },
      "source": [
        "bigram_gi.most_similar('self consciousness')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('common', 0.25377702713012695),\n",
              " ('per ception', 0.24532398581504822),\n",
              " ('efficacy', 0.22992604970932007),\n",
              " ('proclaimed', 0.22748391330242157),\n",
              " ('phaenomena', 0.22655561566352844),\n",
              " ('immense', 0.22575584053993225),\n",
              " ('marked', 0.2227858603000641),\n",
              " ('informs', 0.22088894248008728),\n",
              " ('entrance', 0.22031810879707336),\n",
              " ('fire', 0.2183404564857483)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHajYJP8F7hM"
      },
      "source": [
        "from gensim.models import Phrases\r\n",
        "\r\n",
        "from gensim.models.phrases import Phraser\r\n",
        "\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "documents = list(df[df['school'] == 'german_idealism']['gensim_tokenized'])\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "\r\n",
        "bigram = Phrases(cleaned, min_count=1, threshold=10, delimiter=b' ')\r\n",
        "\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "tokens_list = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens_ = bigram_phraser[sent]\r\n",
        "    tokens_list.append(tokens_)\r\n",
        "\r\n",
        "    # print(tokens_)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ac8e_YyLCtU"
      },
      "source": [
        "tokens_list[200:300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzHUhaYgJ5-r"
      },
      "source": [
        "model = Word2Vec(tokens_list)\r\n",
        "model.train(tokens_list, total_examples=model.corpus_count, epochs=5)\r\n",
        "model_wv = model.wv"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33ioCBY-Klec"
      },
      "source": [
        "len(model_wv.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo5MWIhlMJ8O",
        "outputId": "e7d18efa-13b2-42b0-8d54-17eac48fe17f"
      },
      "source": [
        "model_wv.most_similar('dialectic')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('antinomy', 0.780921220779419),\n",
              " ('paralogism', 0.7721890211105347),\n",
              " ('procedure', 0.7603916525840759),\n",
              " ('resolution', 0.757012128829956),\n",
              " ('critique', 0.7568923234939575),\n",
              " ('deduction', 0.7457656860351562),\n",
              " ('exposition', 0.7319774031639099),\n",
              " ('discipline', 0.7255808115005493),\n",
              " ('doctrine', 0.7144776582717896),\n",
              " ('method', 0.7082430720329285)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEwsQPxELyuf"
      },
      "source": [
        "for token in model_wv.vocab:\r\n",
        "  print(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVTnr2EbIlZo"
      },
      "source": [
        "bigram?"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOszviivq4U-",
        "outputId": "f82158fe-d16c-4d80-d956-86be1408a0e9"
      },
      "source": [
        "# make a base model with the preset parameters\r\n",
        "base_model = make_w2v(series = df[df['school'] == 'german_idealism']['gensim_tokenized'], \r\n",
        "                         stopwords=[], seed=20)\r\n",
        "\r\n",
        "len(base_model.vocab)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s0O8pEHrhki",
        "outputId": "d9fbbbde-9ebf-4ecb-be43-c470593f4b3d"
      },
      "source": [
        "test_w2v_pos_neg(base_model, pairs_to_try)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- posteriori (0.22859)\n",
            "- wisdom (0.22461)\n",
            "- youth (0.21371)\n",
            "- ofa (0.21329)\n",
            "- originates (0.20798)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- paradoxical (0.26606)\n",
            "- rose (0.24617)\n",
            "- position (0.24103)\n",
            "- acted (0.23753)\n",
            "- imply (0.23307)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- cavalieri (0.26471)\n",
            "- meant (0.23451)\n",
            "- magnet (0.23212)\n",
            "- republic (0.22457)\n",
            "- ofan (0.22394)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- nothingness (0.23737)\n",
            "- principal (0.22565)\n",
            "- indirect (0.22187)\n",
            "- pretends (0.22135)\n",
            "- quences (0.2203)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS8zfzZHrhNe"
      },
      "source": [
        "##### Trying Skip-gram instead of C-bow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiV-E9xzrhCL"
      },
      "source": [
        "# make a base model with the preset parameters\r\n",
        "skip_gi_wv = make_w2v_v2(series = df[df['school'] == 'german_idealism']['gensim_tokenized'], \r\n",
        "                         stopwords=[], sg=1, seed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS_Jzbgirg1y",
        "outputId": "ea445696-8a24-48f1-ae43-aeea085712b0"
      },
      "source": [
        "test_w2v_pos_neg(skip_gi_wv, pairs_to_try)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- oriental (0.24547)\n",
            "- variability (0.24501)\n",
            "- confirm (0.24019)\n",
            "- surface (0.23287)\n",
            "- accepted (0.22953)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- minate (0.24381)\n",
            "- imperfection (0.23867)\n",
            "- interest (0.23587)\n",
            "- concentrated (0.22921)\n",
            "- wish (0.22456)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- decreased (0.2668)\n",
            "- bedeutet (0.23926)\n",
            "- motive (0.23033)\n",
            "- discovery (0.22896)\n",
            "- intuited (0.22755)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- subtle (0.2726)\n",
            "- previous (0.26817)\n",
            "- buta (0.25331)\n",
            "- skilled (0.24676)\n",
            "- tongue (0.23751)\n",
            "\n",
            "Positive - ['form']\tNegative - ['content']\n",
            "- information (0.2566)\n",
            "- therefore (0.24964)\n",
            "- plausible (0.24919)\n",
            "- obstinate (0.2391)\n",
            "- orders (0.22979)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnNnqhueyaK-"
      },
      "source": [
        "These seem mildy more sensible. Let's tweak the other parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAv1PjLD7ZSh"
      },
      "source": [
        "##### Parameter Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Y3AggQyaVD",
        "outputId": "ac280378-e653-49e3-9888-fefa4bdee6e3"
      },
      "source": [
        "model_v1 = make_w2v_v2(df[df['school'] == 'german_idealism']['gensim_tokenized'],\r\n",
        "                       stopwords=[],\r\n",
        "                       size=500,\r\n",
        "                       window=5,\r\n",
        "                       min_count=25,\r\n",
        "                       epochs=10,\r\n",
        "                       sg=1, \r\n",
        "                       seed=45)\r\n",
        "\r\n",
        "len(model_v1.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "SVVqIAf8yadZ",
        "outputId": "4720c0e3-8f58-4441-a307-c92e443f5a75"
      },
      "source": [
        "test_w2v_pos_neg(model_v1, pairs_to_try)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-cf252aec3d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_w2v_pos_neg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_to_try\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_v1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn8zvu2z2U_o"
      },
      "source": [
        "Despite tweaking parameters far and wide, it's difficult to get any results that are compellingly sensible. In most cases there are one or two terms in the similarity list that make some sense but others that are just strange or unconnected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7J7_WOz2VK8"
      },
      "source": [
        "#### Trying Another School"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "rZdUtK642VcE",
        "outputId": "9cccd29a-4571-4ea7-d268-1d7049f4dad5"
      },
      "source": [
        "cm_w2v = make_w2v_v2(df[df['school'] == 'communism']['gensim_tokenized'],\r\n",
        "                       stopwords=[],\r\n",
        "                       size=700,\r\n",
        "                       window=10,\r\n",
        "                       min_count=10,\r\n",
        "                       epochs=25,\r\n",
        "                       sg=1, \r\n",
        "                       seed=10)\r\n",
        "\r\n",
        "type(cm_w2v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-479841f683bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                        \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                        seed=10)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm_w2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: make_w2v_v2() got an unexpected keyword argument 'sg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYoO0ozP2VmK"
      },
      "source": [
        "pairs_to_try=[(['material', 'conditions'], []),\r\n",
        "              (['worker'], ['owner']),\r\n",
        "              (['alienation', 'labor'], []),\r\n",
        "              (['capital'], [])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1809IvVx2Vuy",
        "outputId": "5334b4ea-1db1-4c5a-b71a-ed6369e91d63"
      },
      "source": [
        "test_w2v_pos_neg(cm_w2v, pairs_to_try)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['material', 'conditions']\tNegative - []\n",
            "- corruption (0.14189)\n",
            "- treats (0.12078)\n",
            "- forth (0.11571)\n",
            "- loaf (0.11047)\n",
            "- coming (0.10851)\n",
            "\n",
            "Positive - ['worker']\tNegative - ['owner']\n",
            "- putting (0.12181)\n",
            "- subservience (0.12126)\n",
            "- incorporated (0.11326)\n",
            "- legal (0.11286)\n",
            "- under (0.11005)\n",
            "\n",
            "Positive - ['alienation', 'labor']\tNegative - []\n",
            "- transient (0.12955)\n",
            "- selection (0.11778)\n",
            "- regulate (0.11776)\n",
            "- anarchy (0.11498)\n",
            "- sum (0.11169)\n",
            "\n",
            "Positive - ['capital']\tNegative - []\n",
            "- egypt (0.13466)\n",
            "- obliged (0.13243)\n",
            "- mode (0.1267)\n",
            "- ways (0.11922)\n",
            "- turns (0.11891)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hUYWlYr2V3L"
      },
      "source": [
        "Here the results were similar - a few words that made some sense and plenty that were just odd.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5ZnODM2V_P"
      },
      "source": [
        "### Building on top of an Existing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAOaelq2WIr"
      },
      "source": [
        "We'll import GloVe vectors as w2v, then use those as a base from which to train new vectors that are tuned to our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcFZMRT82WRC"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
        "\r\n",
        "glove_file = datapath('/gdrive/MyDrive/Colab_Projects/Phil_NLP/glove.6B.50d.txt')\r\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\r\n",
        "\r\n",
        "_ = glove2word2vec(glove_file, tmp_file)\r\n",
        "\r\n",
        "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjdDWtd72Tli",
        "outputId": "09015b59-ba89-481a-c62e-16f417f6928e"
      },
      "source": [
        "# check out how GloVe works on our test pairs\r\n",
        "test_w2v_pos_neg(glove_vectors, pairs_to_try)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- morality (0.82654)\n",
            "- legal (0.82652)\n",
            "- laws (0.81529)\n",
            "- constitutional (0.80616)\n",
            "- fundamental (0.80217)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- sense (0.83446)\n",
            "- mind (0.79755)\n",
            "- vision (0.78202)\n",
            "- belief (0.78031)\n",
            "- life (0.77984)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- hegelian (0.88376)\n",
            "- dialectical (0.83417)\n",
            "- dialectics (0.80672)\n",
            "- materialist (0.77674)\n",
            "- metaphysics (0.77488)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- reasoning (0.81405)\n",
            "- intuitionistic (0.76531)\n",
            "- concepts (0.75831)\n",
            "- logical (0.75604)\n",
            "- theory (0.75026)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFyauZoo7KW"
      },
      "source": [
        "Ok these make a lot more sense right from the start. But we want them to be trained on our actual philosophical texts - that way we can see how different thinkers use different words and potentially use the vectors for classification.\r\n",
        "\r\n",
        "So in the cells below we train the existing GloVe model on on the German Idealist texts as a test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaCY48wJqsBj",
        "outputId": "34afb53f-7b27-4cb5-9812-8f42fcc35857"
      },
      "source": [
        "glove_vectors.most_similar('freedom')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rights', 0.8569907546043396),\n",
              " ('democracy', 0.8209146857261658),\n",
              " ('respect', 0.8153771162033081),\n",
              " ('freedoms', 0.8151387572288513),\n",
              " ('independence', 0.7843738794326782),\n",
              " ('equality', 0.7748997211456299),\n",
              " ('solidarity', 0.7689430713653564),\n",
              " ('dignity', 0.7647867202758789),\n",
              " ('conscience', 0.7612083554267883),\n",
              " ('movement', 0.7480580806732178)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSOA8Zz2o7vX"
      },
      "source": [
        "# isolate the relevant school\r\n",
        "documents = df[df['school'] == 'german_idealism']['gensim_tokenized']\r\n",
        "\r\n",
        "# get bigrams for that school\r\n",
        "stopwords = []\r\n",
        "\r\n",
        "sentences = [sentence for sentence in documents]\r\n",
        "cleaned = []\r\n",
        "for sentence in sentences:\r\n",
        "  cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "  cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "  cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "\r\n",
        "bigram = Phrases(cleaned, min_count=20, threshold=10, delimiter=b' ')\r\n",
        "\r\n",
        "bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "tokens_list = []\r\n",
        "for sent in cleaned:\r\n",
        "    tokens_ = bigram_phraser[sent]\r\n",
        "    tokens_list.append(tokens_)\r\n",
        "\r\n",
        "# build a toy model to update with\r\n",
        "base_model = Word2Vec(size=300, min_count=1)\r\n",
        "base_model.build_vocab(tokens_list)\r\n",
        "total_examples = base_model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "base_model.train(tokens_list, total_examples=total_examples, epochs=base_model.epochs)\r\n",
        "base_model_wv = base_model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkoT5kMeqx5s"
      },
      "source": [
        "base_model_wv.most_similar(positive=['faculty'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0FjXahncQya"
      },
      "source": [
        "test_w2v(base_model_wv, pairs_to_try)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3MnHFbcl7d"
      },
      "source": [
        "We can immediately see that these make a lot more sense (and the similarity scores are a lot higher). Self-consciousness is commonly associated with freedom in German idealism, logic with metaphysics, and the moral law with universality and the good. This is a massive improvement - these vectors can be fairly said to reflect how german idealists use these terms. Moreover, they are significantly different than the original GloVe model, which indicates that there was real learning going on here.\r\n",
        "\r\n",
        "For comparison, let's check these same terms, but as used by Phenomenologists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s1Jkkc7dqk9"
      },
      "source": [
        "def train_glove(school, glove_vectors, threshold=10, stopwords=[],\r\n",
        "                min_count=10):\r\n",
        "  # isolate the relevant school\r\n",
        "  sentences = df[df['school'] == school]['gensim_tokenized']\r\n",
        "  sentences = [sentence for sentence in documents]\r\n",
        "  cleaned = []\r\n",
        "  for sentence in sentences:\r\n",
        "    cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "    cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "    cleaned.append(cleaned_sentence)\r\n",
        "\r\n",
        "  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, delimiter=b' ')\r\n",
        "  bigram_phraser = Phraser(bigram)\r\n",
        "\r\n",
        "  bigramed_tokens = []\r\n",
        "  for sent in cleaned:\r\n",
        "      tokens = bigram_phraser[sent]\r\n",
        "      bigramed_tokens.append(tokens)\r\n",
        "\r\n",
        "  # built a toy model to update with\r\n",
        "  model = Word2Vec(size=300, min_count=1)\r\n",
        "  model.build_vocab(bigramed_tokens)\r\n",
        "  total_examples = model.corpus_count\r\n",
        "\r\n",
        "  # add GloVe's vocabulary & weights\r\n",
        "  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "  # train on our data\r\n",
        "  model.train(bigramed_tokens, total_examples=total_examples, epochs=model.epochs)\r\n",
        "  return model.wv\r\n",
        "\r\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIMLCNVHdxID",
        "outputId": "de96087f-295a-4ad7-bf44-bb8a6a4595cb"
      },
      "source": [
        "ph_model = train_glove(school='phenomenologoy', glove_vectors=glove_vectors)\r\n",
        "\r\n",
        "test_w2v(ph_model, pairs_to_try)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- con (0.96696)\n",
            "- universal (0.95497)\n",
            "- per (0.94912)\n",
            "- power (0.94234)\n",
            "- whole (0.94027)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- self consciousness (0.95071)\n",
            "- essence (0.94981)\n",
            "- reality (0.94393)\n",
            "- purpose (0.94244)\n",
            "- individuality (0.9378)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- presentation (0.95525)\n",
            "- community (0.95516)\n",
            "- synthesis (0.94867)\n",
            "- development (0.94676)\n",
            "- division (0.94583)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- common sense (0.94504)\n",
            "- taste (0.94017)\n",
            "- judgement (0.93902)\n",
            "- metaphysics (0.93571)\n",
            "- man (0.93381)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ZdfxiSqQAS"
      },
      "source": [
        "ph_model.most_similar('dasein')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRp6eNmHeP01"
      },
      "source": [
        "These are substantially different - which is good since the schools are substantially different. They mostly make sense, except for words like 'dialectic' which are rarely used by phenomenologists. The general attitude towards logic as a cold sterilizing force is evident, as is their emphasis on perception and natural life, as compared to the German idealist emphasis on abstractions like universality and freedom (see 'self consciousness'). \r\n",
        "\r\n",
        "These vectors seem to be an effective tool for revealing word usage between the schools. \r\n",
        "\r\n",
        "As a final kind of exploration of this method, we'll train w2v models in this way for each school and examine how each of them looks a couple of the same words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "R8_XI24ogh17",
        "outputId": "a0a857b0-29c1-4e01-b538-06df92cb2386"
      },
      "source": [
        "w2v_dict = {}\r\n",
        "\r\n",
        "for school in df['school'].unique():\r\n",
        "  w2v_dict[school] = train_glove(school, glove_vectors=glove_vectors)\r\n",
        "  print(f'{school} completed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-6a49bb629386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mschool\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'school'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mw2v_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mschool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglove_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{school} completed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-4f22cf05dbd6>\u001b[0m in \u001b[0;36mtrain_glove\u001b[0;34m(school, glove_vectors)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# add GloVe's vocabulary & weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# train on our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m    942\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1822\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m             \u001b[0mnewvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0;31m# Raise an error if an online update is run before initial training on a corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mseeded_vector\u001b[0;34m(self, seed_string, vector_size)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;34m\"\"\"Get a random vector (but deterministic by seed_string).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0;31m# Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         \u001b[0monce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xffffffff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_mt19937.pyx\u001b[0m in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mbit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random.bit_generator.BitGenerator.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mbit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random.bit_generator.SeedSequence.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mbit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random.bit_generator.SeedSequence.get_assembled_entropy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mbit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random.bit_generator._coerce_to_uint32_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mbit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random.bit_generator._int_to_uint32_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZktiqighpT",
        "outputId": "7beea02e-95a0-441e-c7b6-2fc49df106fa"
      },
      "source": [
        "for school in df['school'].unique():\r\n",
        "  print(f'\\t{school.upper()}')\r\n",
        "  print('----------------------')\r\n",
        "  test_w2v(w2v_dict[school], [(['philosophy'], [])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPLATO\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- friendship (0.96654)\n",
            "- war (0.94734)\n",
            "- learning (0.9461)\n",
            "- water (0.94579)\n",
            "- virtue (0.94529)\n",
            "\n",
            "\tARISTOTLE\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- conception (0.90234)\n",
            "- generation (0.89594)\n",
            "- respiration (0.87903)\n",
            "- refrigeration (0.87197)\n",
            "- locomotion (0.86564)\n",
            "\n",
            "\tEMPIRICISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- religion (0.959)\n",
            "- morality (0.95895)\n",
            "- infinity (0.94697)\n",
            "- mankind (0.94397)\n",
            "- state (0.93667)\n",
            "\n",
            "\tRATIONALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- necessity (0.94119)\n",
            "- physician (0.93936)\n",
            "- reasoning (0.93735)\n",
            "- indifference (0.9352)\n",
            "- aside (0.93257)\n",
            "\n",
            "\tANALYTIC\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- science (0.95345)\n",
            "- logic (0.94979)\n",
            "- psychology (0.94333)\n",
            "- speech (0.93447)\n",
            "- semantics (0.92426)\n",
            "\n",
            "\tCONTINENTAL\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- criticism (0.97731)\n",
            "- unreason (0.97729)\n",
            "- metaphysics (0.96931)\n",
            "- knowledge (0.96654)\n",
            "- medicine (0.96633)\n",
            "\n",
            "\tPHENOMENOLOGY\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- consciousness (0.97946)\n",
            "- science (0.97886)\n",
            "- psychology (0.9779)\n",
            "- history (0.97396)\n",
            "- knowledge (0.9739)\n",
            "\n",
            "\tGERMAN_IDEALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- morality (0.91712)\n",
            "- theology (0.91276)\n",
            "- physics (0.90493)\n",
            "- science (0.90184)\n",
            "- logic (0.89716)\n",
            "\n",
            "\tCOMMUNISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- conceptions (0.95942)\n",
            "- listok (0.95153)\n",
            "- energy (0.94135)\n",
            "- tenant (0.94035)\n",
            "- penury (0.9398)\n",
            "\n",
            "\tCAPITALISM\n",
            "----------------------\n",
            "Positive - ['philosophy']\tNegative - []\n",
            "- commerce (0.95785)\n",
            "- scale (0.95375)\n",
            "- various (0.95342)\n",
            "- religion (0.95242)\n",
            "- necessaries (0.95235)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmP9U0WrghgT"
      },
      "source": [
        "Interestingly, many of these top words align quite strongly with the school's general attitude towards philosophy. \r\n",
        "\r\n",
        "The model seems solid - our next step is to train one on the entire corpus for use in classification. We do that, and export it, below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A-9hc_CghWr"
      },
      "source": [
        "# isolate the relevant school\r\n",
        "sentences = df['gensim_tokenized']\r\n",
        "\r\n",
        "# built a toy model to update with\r\n",
        "model = Word2Vec(size=300, min_count=1)\r\n",
        "model.build_vocab(sentences)\r\n",
        "total_examples = model.corpus_count\r\n",
        "\r\n",
        "# add GloVe's vocabulary & weights\r\n",
        "model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\r\n",
        "\r\n",
        "# train on our data\r\n",
        "model.train(sentences, total_examples=total_examples, epochs=model.epochs)\r\n",
        "all_text_wv = model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6rnhbD235n"
      },
      "source": [
        "all_text_wv.save_word2vec_format('/gdrive/MyDrive/Colab_Projects/Phil_NLP/w2v_for_nn2.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iESlNMbJgguy"
      },
      "source": [
        "from gensim.models import Phrases\r\n",
        ">>>\r\n",
        "# Train a bigram detector.\r\n",
        "bigram_transformer = Phrases(common_texts)\r\n",
        ">>>\r\n",
        "# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\r\n",
        "model = Word2Vec(bigram_transformer[common_texts], min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w39c1Eg24cq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnn_I6qb26LP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbH9Rszb26Uh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQRVX-2D26dR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5BYc93p26l1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8A0q37-26uO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCyJPbCI260-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQB1erN24kN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUmygxss24tD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkbBQwL12412"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "-A9RVhlnZWCo",
        "outputId": "5578da8d-c1b4-498d-a91c-f29e7c9b66fa"
      },
      "source": [
        "model_2.intersect_word2vec_format('test_word2vec.txt', binary=False, lockf=1.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e54bb4decb69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_word2vec.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mintersect_word2vec_format\u001b[0;34m(self, fname, lockf, binary, encoding, unicode_errors)\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0moverlap_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_word2vec.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYtZ5ZwaZD-I",
        "outputId": "0f27f9d4-20fd-4f2e-d33e-2b7062412865"
      },
      "source": [
        "model_2.intersect_word2vec_format"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBiczb5oveCO",
        "outputId": "d035a2b0-3437-447c-e184-b4cffb0ef1d4"
      },
      "source": [
        "test_w2v_pos_neg(vectors, pairs_to_try)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- good (0.91925)\n",
            "- negative (0.9187)\n",
            "- universal (0.91712)\n",
            "- fine (0.91255)\n",
            "- public (0.91191)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- essence (0.93576)\n",
            "- freedom (0.91574)\n",
            "- universality (0.91299)\n",
            "- quantum (0.9107)\n",
            "- reflection (0.90237)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- division (0.95115)\n",
            "- process (0.95111)\n",
            "- method (0.94972)\n",
            "- definition (0.94837)\n",
            "- deduction (0.93935)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- science (0.93664)\n",
            "- metaphysics (0.92867)\n",
            "- teleology (0.92438)\n",
            "- taste (0.9074)\n",
            "- theology (0.90333)\n",
            "\n",
            "Positive - ['form']\tNegative - ['content']\n",
            "- ﬂag (0.54249)\n",
            "- conducts (0.53321)\n",
            "- proofloo (0.49985)\n",
            "- semester (0.4997)\n",
            "- rsality (0.4979)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yzRubLD7tGyR",
        "outputId": "999f98c6-8f2e-47dc-af5e-2768d24209c2"
      },
      "source": [
        "Word2Vec.load('word_vectors.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'vocabulary'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8bf50bc41e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word_vectors.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/word2vec.py\u001b[0m in \u001b[0;36mload_old_word2vec\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mold_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vector_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     params = {\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m         \u001b[0;31m# update older models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/old_saveload.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/old_saveload.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    378\u001b[0m             b'gensim.models.wrappers.fasttext', b'gensim.models.deprecated.fasttext_wrapper')\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.deprecated.keyedvectors' from '/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/keyedvectors.py'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaxMWdv8tPS-"
      },
      "source": [
        "glove_vectors.save('word_vectors.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "B4WLPQCosuhx",
        "outputId": "deff5f9a-90c1-422a-81ab-3ebb53abc1d2"
      },
      "source": [
        "new_model = Word2Vec.load(glove_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36m_adapt_by_suffix\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \"\"\"\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'npz'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.gz'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.bz2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'endswith'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4388fa7825d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/word2vec.py\u001b[0m in \u001b[0;36mload_old_word2vec\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_old_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mold_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vector_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     params = {\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m         \u001b[0;31m# update older models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/old_saveload.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s object from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/old_saveload.py\u001b[0m in \u001b[0;36m_adapt_by_suffix\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \"\"\"\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'npz'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.gz'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.bz2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BgkVA1ao7Tr"
      },
      "source": [
        "# get all the words from the german idealist corpus\r\n",
        "german_idealist_words = []\r\n",
        "for sentence in df[df['school'] == 'german_idealism']['gensim_tokenized']:\r\n",
        "  for word in sentence:\r\n",
        "    german_idealist_words.append(word)\r\n",
        "\r\n",
        "german_idealist_words = list(set(german_idealist_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "sfE5qIl7o7db",
        "outputId": "753a6c62-0049-45ec-abcd-29213e17b1e5"
      },
      "source": [
        "glove_vectors.build_vocab([german_idealist_words], update=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0c95ceb08e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgerman_idealist_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'build_vocab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceMYAyboV-1u",
        "outputId": "aaa12490-4836-49fd-fd74-9a1422edc4ed"
      },
      "source": [
        "glove_vectors.intersect?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `glove_vectors.intersect` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "-kqlShr9o7lv",
        "outputId": "c1ac7ce2-2d23-42d5-83c9-1741b67880ca"
      },
      "source": [
        "glove_vectors.train(df[df['school'] == 'german_idealism']['gensim_tokenized'], \r\n",
        "                    min_count=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-712d9d82adef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m glove_vectors.train(df[df['school'] == 'german_idealism']['gensim_tokenized'], \n\u001b[0m\u001b[1;32m      2\u001b[0m                     min_count=1)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaNvFbAxo74_"
      },
      "source": [
        "glove_file = datapath('/gdrive/MyDrive/Colab_Projects/Phil_NLP/glove.6B.50d.txt')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTTXg8Ngo8CD"
      },
      "source": [
        "vectors = model_2.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCVDTqePvdft",
        "outputId": "7e0dc015-5f51-4731-d69f-80d535241e42"
      },
      "source": [
        "test_w2v_pos_neg(glove_vectors, pairs_to_try)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive - ['law', 'moral']\tNegative - []\n",
            "- morality (0.82654)\n",
            "- legal (0.82652)\n",
            "- laws (0.81529)\n",
            "- constitutional (0.80616)\n",
            "- fundamental (0.80217)\n",
            "\n",
            "Positive - ['self', 'consciousness']\tNegative - []\n",
            "- sense (0.83446)\n",
            "- mind (0.79755)\n",
            "- vision (0.78202)\n",
            "- belief (0.78031)\n",
            "- life (0.77984)\n",
            "\n",
            "Positive - ['dialectic']\tNegative - []\n",
            "- hegelian (0.88376)\n",
            "- dialectical (0.83417)\n",
            "- dialectics (0.80672)\n",
            "- materialist (0.77674)\n",
            "- metaphysics (0.77488)\n",
            "\n",
            "Positive - ['logic']\tNegative - []\n",
            "- reasoning (0.81405)\n",
            "- intuitionistic (0.76531)\n",
            "- concepts (0.75831)\n",
            "- logical (0.75604)\n",
            "- theory (0.75026)\n",
            "\n",
            "Positive - ['form']\tNegative - ['content']\n",
            "- succession (0.60184)\n",
            "- 13th (0.60066)\n",
            "- 17th (0.59596)\n",
            "- 16th (0.58968)\n",
            "- 18th (0.58704)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBzuQlMLvdHs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIsgjlpovdA3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0sTYjhS2Tul"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvKO9KANvaZz"
      },
      "source": [
        "def make_w2v(series, stopwords=None, size=200, window=5, min_count=5, workers=-1, \n",
        "             epochs=20, lowercase=True):\n",
        "    sentences = series.map(word_tokenize)\n",
        "    if lowercase:\n",
        "      cleaned_sentences = []\n",
        "      for sentence in list(sentences):\n",
        "          cleaned = [x.lower() for x in sentence if x.lower() not in stopwords]\n",
        "          cleaned_sentences.append(cleaned)\n",
        "    else:\n",
        "      cleaned = [word for word in sentences not in stopwords]\n",
        "      cleaned_sentences.append(cleaned)\n",
        "    model = Word2Vec(cleaned_sentences, size=size, window=window, \n",
        "                     min_count=min_count, workers=workers, seed=17)\n",
        "    model.train(cleaned_sentences, total_examples=model.corpus_count, epochs=epochs)\n",
        "    model_wv = model.wv\n",
        "    del model\n",
        "    return model_wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkKu4_RUIBLF"
      },
      "source": [
        "def make_w2v_v2(series, stopwords=None, size=200, window=5, min_count=5, workers=-1, \r\n",
        "             epochs=20, lowercase=True):\r\n",
        "    sentences = [sentence for sentence in series]\r\n",
        "    if lowercase:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word.lower() for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "    else:\r\n",
        "      cleaned = []\r\n",
        "      for sentence in sentences:\r\n",
        "        cleaned_sentence = [word for word in sentence]\r\n",
        "        cleaned_sentence = [word for word in sentence if word not in stopwords]\r\n",
        "        cleaned.append(cleaned_sentence)\r\n",
        "    model = Word2Vec(cleaned, size=size, window=window, \r\n",
        "                     min_count=min_count, workers=workers, seed=17)\r\n",
        "    model.train(series, total_examples=model.corpus_count, epochs=epochs)\r\n",
        "    model_wv = model.wv\r\n",
        "    del model\r\n",
        "    return model_wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Pl2CfbwNvt"
      },
      "source": [
        "gi_wv = make_w2v(df.loc[df['school'] == 'communism']['sentence_str'], \n",
        "                 stopwords=[],\n",
        "                 size=700,\n",
        "                 epochs=200,\n",
        "                 window=7,\n",
        "                 min_count=10, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X44l6EqgwWkU",
        "outputId": "ca707290-d250-495f-b2e9-57b4ed6ab5b2"
      },
      "source": [
        "gi_wv.most_similar('money')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('vital', 0.12160132080316544),\n",
              " ('resort', 0.11713527143001556),\n",
              " ('breaking', 0.11627447605133057),\n",
              " ('notes', 0.11610057204961777),\n",
              " ('creates', 0.11516699194908142),\n",
              " ('boys', 0.10976336896419525),\n",
              " ('cotton', 0.10716287791728973),\n",
              " ('completion', 0.10459037870168686),\n",
              " ('wide', 0.10393355041742325),\n",
              " ('concerns', 0.10353465378284454)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j-ZEdeZNCVy"
      },
      "source": [
        "gi_wv = make_w2v(df.loc[df['school'] == 'capitalism']['sentence_str'], \r\n",
        "                 stopwords=[],\r\n",
        "                 size=100,\r\n",
        "                 epochs=25,\r\n",
        "                 window=3,\r\n",
        "                 min_count=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om5JOUQhNERB",
        "outputId": "dae840b1-c928-4c1f-efa7-2e05bcee96ce"
      },
      "source": [
        "gi_wv.most_similar('money')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tendency', 0.3689945638179779),\n",
              " ('continues', 0.36447644233703613),\n",
              " ('before', 0.3537063002586365),\n",
              " ('lowered', 0.3263741731643677),\n",
              " ('met', 0.3160112202167511),\n",
              " ('separately', 0.3103809356689453),\n",
              " ('monied', 0.303830087184906),\n",
              " ('barrenness', 0.29618436098098755),\n",
              " ('speaking', 0.29251614212989807),\n",
              " ('vol', 0.286594033241272)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhYPJOXvBNh_"
      },
      "source": [
        "pl_wv = make_w2v_v2(df.loc[df['school'] == 'plato']['tokenized'], \r\n",
        "                 stopwords=[],\r\n",
        "                 size=50,\r\n",
        "                 epochs=25,\r\n",
        "                 window=5,\r\n",
        "                 min_count=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "BQZw7kfNBUQl",
        "outputId": "074830ac-373f-463e-b5fb-a21e8c15b447"
      },
      "source": [
        "pl_wv.most_similar('concept')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-898659328a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpl_wv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'concept'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'concept' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uTu-1xZmMdb"
      },
      "source": [
        "model = Word2Vec(list(w2v_data), size=700, window=8, min_count=50, workers=-1, seed=17)\n",
        "\n",
        "model.train(w2v_data, total_examples=model.corpus_count, epochs=10)\n",
        "\n",
        "wv = model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpjTpnOlfOHP",
        "outputId": "33e750cf-2135-4d77-d48b-07384c7f9eae"
      },
      "source": [
        "df.loc[df['school'] == 'plato']['sentence_str'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     What new, Socrates, to make you leave your us...\n",
              "1    Surely you are not prosecuting anyone before t...\n",
              "2    The Athenians do not call this a prosecution b...\n",
              "3                                What is this you say?\n",
              "4    Someone must have indicted you, for you are no...\n",
              "Name: sentence_str, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdY0Ffk0fRM7",
        "outputId": "2f777f94-491f-4f14-ca6e-40a1d6181976"
      },
      "source": [
        "df.loc[df['school'] == 'german_idealism']['sentence_str'].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "248940    Why this Critiq is titled a critique not of pu...\n",
              "248941    This Critique is to establish mere that there ...\n",
              "248942    If it succeeds in this berhaupt I render this ...\n",
              "248943    because the latter can too often be misread as...\n",
              "248944    In the few cases where 'as such' is used to tr...\n",
              "Name: sentence_str, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDihrIQI0m8W"
      },
      "source": [
        "def most_similar(word, topn=5):\r\n",
        "    word = nlp.vocab[str(word)]\r\n",
        "    queries = [\r\n",
        "        w for w in word.vocab \r\n",
        "        if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\r\n",
        "    ]\r\n",
        "\r\n",
        "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\r\n",
        "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vsw9ypR0ms7",
        "outputId": "26f52850-e5fa-4669-c0bb-093c785d8453"
      },
      "source": [
        "most_similar('substance')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('substances', 0.7708152),\n",
              " ('alcohol', 0.6000577),\n",
              " ('drug', 0.5985114),\n",
              " ('drugs', 0.5792217),\n",
              " ('chemical', 0.57279444)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TVrAxf0nNZc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OERPOU5nNPL"
      },
      "source": [
        "from sklearn.utils import class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "VvOGsiyXptns",
        "outputId": "efc02af3-056d-475f-e752-2176fa795e4e"
      },
      "source": [
        "y = pd.get_dummies(df['school'])\r\n",
        "y.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>analytic</th>\n",
              "      <th>aristotle</th>\n",
              "      <th>capitalism</th>\n",
              "      <th>communism</th>\n",
              "      <th>continental</th>\n",
              "      <th>empiricism</th>\n",
              "      <th>german_idealism</th>\n",
              "      <th>phenomenology</th>\n",
              "      <th>plato</th>\n",
              "      <th>rationalism</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   analytic  aristotle  capitalism  ...  phenomenology  plato  rationalism\n",
              "0         0          0           0  ...              0      1            0\n",
              "1         0          0           0  ...              0      1            0\n",
              "2         0          0           0  ...              0      1            0\n",
              "3         0          0           0  ...              0      1            0\n",
              "4         0          0           0  ...              0      1            0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPnEfVJsrnM1",
        "outputId": "6e5255aa-afa1-44f7-9856-c5254dbc2966"
      },
      "source": [
        "y_classes = y.idxmax(1, skipna=False)\r\n",
        "y_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0              plato\n",
              "1              plato\n",
              "2              plato\n",
              "3              plato\n",
              "4              plato\n",
              "             ...    \n",
              "327667    capitalism\n",
              "327668    capitalism\n",
              "327669    capitalism\n",
              "327670    capitalism\n",
              "327671    capitalism\n",
              "Length: 327672, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiHd5lNNrtAx"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "y_classes = y_train.idxmax(1, skipna=False)\r\n",
        "\r\n",
        "# Instantiate the label encoder\r\n",
        "le = LabelEncoder()\r\n",
        "\r\n",
        "# Fit the label encoder to our label series\r\n",
        "le.fit(list(y_classes))\r\n",
        "\r\n",
        "# Create integer based labels Series\r\n",
        "y_integers = le.transform(list(y_classes))\r\n",
        "\r\n",
        "# Create dict of labels : integer representation\r\n",
        "labels_and_integers = dict(zip(y_classes, y_integers))\r\n",
        "\r\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\r\n",
        "\r\n",
        "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\r\n",
        "sample_weights = compute_sample_weight('balanced', y_integers)\r\n",
        "\r\n",
        "class_weights_dict = dict(zip(le.transform(list(le.classes_)), class_weights))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltzKgCovnNEz"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df['sentence_str'], df['school'])\r\n",
        "y_train = pd.get_dummies(y_train)\r\n",
        "class_weights = class_weight.compute_class_weight('balanced',\r\n",
        "                                                 np.unique(y_train),\r\n",
        "                                                 y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKhlP4SrnMy8",
        "outputId": "8798ad39-1c3f-46fc-e500-a2e49fea529f"
      },
      "source": [
        "class_weights_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.6119067775509188,\n",
              " 1: 0.6455997478064415,\n",
              " 2: 1.7993410455410748,\n",
              " 3: 1.7998681704994872,\n",
              " 4: 0.9674592551767577,\n",
              " 5: 1.6027783212678537,\n",
              " 6: 0.7702679830747532,\n",
              " 7: 1.13891000092687,\n",
              " 8: 0.8537275064267352,\n",
              " 9: 1.4279721092388147}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jglwoMGnMqi",
        "outputId": "982e7369-df1b-44b4-b6ee-eea45f0bbdc5"
      },
      "source": [
        "labels_and_integers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'analytic': 0,\n",
              " 'aristotle': 1,\n",
              " 'capitalism': 2,\n",
              " 'communism': 3,\n",
              " 'continental': 4,\n",
              " 'empiricism': 5,\n",
              " 'german_idealism': 6,\n",
              " 'phenomenology': 7,\n",
              " 'plato': 8,\n",
              " 'rationalism': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEN3-ISLwfPn"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, LSTM, Embedding\n",
        "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Sequential\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.preprocessing import text, sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDRzRhw-y7qL"
      },
      "source": [
        "y_train = pd.get_dummies(df['school']).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmk_ZLu2y_Vn"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "list_tokenized_headlines = tokenizer.texts_to_sequences(x_train)\n",
        "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=420)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSZqQNfrzeQD"
      },
      "source": [
        "model = Sequential()\n",
        "embedding_size = 128\n",
        "model.add(Embedding(len(tokenizer.word_index)+1, embedding_size))\n",
        "# model.add(glove_vectors.get_keras_embedding())\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(50, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "VohaZEB_11DD",
        "outputId": "c44c4a68-9f76-430f-b3b0-47cc713159c7"
      },
      "source": [
        "history = model.fit(X_t, y_train, epochs=10, batch_size=128, validation_split=0.2, class_weight=class_weights_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1536/1536 [==============================] - 146s 94ms/step - loss: 2.3010 - accuracy: 0.1014 - val_loss: 2.3003 - val_accuracy: 0.0975\n",
            "Epoch 2/10\n",
            "1536/1536 [==============================] - 143s 93ms/step - loss: 2.2789 - accuracy: 0.1343 - val_loss: 2.3157 - val_accuracy: 0.1035\n",
            "Epoch 3/10\n",
            "1536/1536 [==============================] - 143s 93ms/step - loss: 2.0266 - accuracy: 0.2619 - val_loss: 2.5325 - val_accuracy: 0.0906\n",
            "Epoch 4/10\n",
            "1536/1536 [==============================] - 145s 95ms/step - loss: 1.6577 - accuracy: 0.3836 - val_loss: 2.8058 - val_accuracy: 0.0938\n",
            "Epoch 5/10\n",
            "1536/1536 [==============================] - 145s 95ms/step - loss: 1.4095 - accuracy: 0.4652 - val_loss: 3.0479 - val_accuracy: 0.0945\n",
            "Epoch 6/10\n",
            "1536/1536 [==============================] - 145s 94ms/step - loss: 1.2566 - accuracy: 0.5203 - val_loss: 3.4917 - val_accuracy: 0.0945\n",
            "Epoch 7/10\n",
            "1536/1536 [==============================] - 145s 94ms/step - loss: 1.1408 - accuracy: 0.5590 - val_loss: 3.8444 - val_accuracy: 0.0986\n",
            "Epoch 8/10\n",
            "1536/1536 [==============================] - 146s 95ms/step - loss: 1.0455 - accuracy: 0.5928 - val_loss: 4.0462 - val_accuracy: 0.0963\n",
            "Epoch 9/10\n",
            "1536/1536 [==============================] - 145s 95ms/step - loss: 0.9790 - accuracy: 0.6192 - val_loss: 4.3605 - val_accuracy: 0.0970\n",
            "Epoch 10/10\n",
            " 523/1536 [=========>....................] - ETA: 1:31 - loss: 0.8852 - accuracy: 0.6561"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-0870e8e2482d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIOgqmbz2WJR"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mQGj8ajdqCj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZYpSac8SSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "3171ac8a-3291-465d-8ef3-b8932a5681c6"
      },
      "source": [
        "history_df.plot(figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7ecdf342b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLdq-QpIVmyg"
      },
      "source": [
        "word_list = ['ness', 'pri', 'al', 'dom', 'te', 'bu', 'er', 'critiq', 'educa', \r\n",
        "             'd', 'dx', 'hen', 'principleb', 'na', 'sensati', 'derstanding',\r\n",
        "             'objecti', 'representati', 'sciousness', 'cluding', 'ers', 'simp','jects', \r\n",
        "             'il']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjCSPF968kH7"
      },
      "source": [
        "word_checker = pd.DataFrame()\r\n",
        "for word in word_list:\r\n",
        "  word_check_slice = df[(df['sentence_lowered'].str.contains('\\s'+word+'\\s'.lower()))].copy()\r\n",
        "  word_check_slice['word'] = word\r\n",
        "  word_checker = word_checker.append(word_check_slice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp5uKNA0Vm81",
        "outputId": "3b12833e-aa3f-4b36-8171-6afc0b3d1626"
      },
      "source": [
        "len(word_checker)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z09vVUJePBhJ",
        "outputId": "855733c5-f0d9-43bd-b70b-0734d1aa7537"
      },
      "source": [
        "word_checker['word'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ness           119\n",
              "al              81\n",
              "te              76\n",
              "na              71\n",
              "pri             69\n",
              "il              59\n",
              "er              58\n",
              "sciousness      54\n",
              "hen             49\n",
              "derstanding     24\n",
              "jects           22\n",
              "dx              22\n",
              "principleb      21\n",
              "bu              19\n",
              "ers             10\n",
              "dom              9\n",
              "sensati          5\n",
              "educa            5\n",
              "objecti          5\n",
              "critiq           3\n",
              "cluding          2\n",
              "simp             1\n",
              "d                1\n",
              "Name: word, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U1bCUdSXPBSX",
        "outputId": "02fde3ae-f5c3-4945-85d0-db75bb701be8"
      },
      "source": [
        "word_checker[word_checker['word']=='bu']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>school</th>\n",
              "      <th>sentence_spacy</th>\n",
              "      <th>sentence_str</th>\n",
              "      <th>sentence_length</th>\n",
              "      <th>sentence_lowered</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>162653</th>\n",
              "      <td>184776</td>\n",
              "      <td>quintessence</td>\n",
              "      <td>quine</td>\n",
              "      <td>analytic</td>\n",
              "      <td>a bu the English translation of which has the form 'p and not tJ .</td>\n",
              "      <td>a bu the English translation of which has the form 'p and not tJ .</td>\n",
              "      <td>66</td>\n",
              "      <td>a bu the english translation of which has the form 'p and not tj .</td>\n",
              "      <td>[a, bu, the, English, translation, of, which, has, the, form, ', p, and, not, tJ, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203563</th>\n",
              "      <td>229585</td>\n",
              "      <td>writing and difference</td>\n",
              "      <td>derrida</td>\n",
              "      <td>continental</td>\n",
              "      <td>I can only, I must only speak to the other; that is, I must call him in the vocative, which is not a category, a case of speech, bu rather the bursting forth, the very raising up of speech.</td>\n",
              "      <td>I can only, I must only speak to the other; that is, I must call him in the vocative, which is not a category, a case of speech, bu rather the bursting forth, the very raising up of speech.</td>\n",
              "      <td>189</td>\n",
              "      <td>i can only, i must only speak to the other; that is, i must call him in the vocative, which is not a category, a case of speech, bu rather the bursting forth, the very raising up of speech.</td>\n",
              "      <td>[I, can, only, ,, I, must, only, speak, to, the, other, ;, that, is, ,, I, must, call, him, in, the, vocative, ,, which, is, not, a, category, ,, a, case, of, speech, ,, bu, rather, the, bursting, forth, ,, the, very, raising, up, of, speech, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206754</th>\n",
              "      <td>232922</td>\n",
              "      <td>writing and difference</td>\n",
              "      <td>derrida</td>\n",
              "      <td>continental</td>\n",
              "      <td>The Irish and Welsh custom of the wake is little known, bu was Joyce's last work, Finnegan's Wake, Finnegan's funeral vigil (but the reading of this famous novel is at least uneasy).</td>\n",
              "      <td>The Irish and Welsh custom of the wake is little known, bu was Joyce's last work, Finnegan's Wake, Finnegan's funeral vigil (but the reading of this famous novel is at least uneasy).</td>\n",
              "      <td>182</td>\n",
              "      <td>the irish and welsh custom of the wake is little known, bu was joyce's last work, finnegan's wake, finnegan's funeral vigil (but the reading of this famous novel is at least uneasy).</td>\n",
              "      <td>[The, Irish, and, Welsh, custom, of, the, wake, is, little, known, ,, bu, was, Joyce, 's, last, work, ,, Finnegan, 's, Wake, ,, Finnegan, 's, funeral, vigil, (, but, the, reading, of, this, famous, novel, is, at, least, uneasy, ), .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279425</th>\n",
              "      <td>310777</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>Here thinking, instead of making progress in the transition from Subject to Predicate, in reali ty feels itselfchecked by the loss of the Bu bject, and, missing it, is thrown back on to the thought of the Subject.</td>\n",
              "      <td>Here thinking, instead of making progress in the transition from Subject to Predicate, in reali ty feels itselfchecked by the loss of the Bu bject, and, missing it, is thrown back on to the thought of the Subject.</td>\n",
              "      <td>213</td>\n",
              "      <td>here thinking, instead of making progress in the transition from subject to predicate, in reali ty feels itselfchecked by the loss of the bu bject, and, missing it, is thrown back on to the thought of the subject.</td>\n",
              "      <td>[Here, thinking, ,, instead, of, making, progress, in, the, transition, from, Subject, to, Predicate, ,, in, reali, ty, feels, itselfchecked, by, the, loss, of, the, Bu, bject, ,, and, ,, missing, it, ,, is, thrown, back, on, to, the, thought, of, the, Subject, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279717</th>\n",
              "      <td>311080</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>preservingN ow is, therefore, not immediate bu mediated; for it is determined as a permanent and self preserving Now</td>\n",
              "      <td>preservingN ow is, therefore, not immediate bu mediated; for it is determined as a permanent and self preserving Now</td>\n",
              "      <td>116</td>\n",
              "      <td>preservingn ow is, therefore, not immediate bu mediated; for it is determined as a permanent and self preserving now</td>\n",
              "      <td>[preservingN, ow, is, ,, therefore, ,, not, immediate, bu, mediated, ;, for, it, is, determined, as, a, permanent, and, self, preserving, Now]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280033</th>\n",
              "      <td>311425</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>It advances uninterruptedly to the outcome in which all these essential essentialities or determinations are equally set aside; bu in each single momen tit is conscious only of this one determinateness as the truth, and then in tu~n of the opposite one.</td>\n",
              "      <td>It advances uninterruptedly to the outcome in which all these essential essentialities or determinations are equally set aside; bu in each single momen tit is conscious only of this one determinateness as the truth, and then in tu~n of the opposite one.</td>\n",
              "      <td>253</td>\n",
              "      <td>it advances uninterruptedly to the outcome in which all these essential essentialities or determinations are equally set aside; bu in each single momen tit is conscious only of this one determinateness as the truth, and then in tu~n of the opposite one.</td>\n",
              "      <td>[It, advances, uninterruptedly, to, the, outcome, in, which, all, these, essential, essentialities, or, determinations, are, equally, set, aside, ;, bu, in, each, single, momen, tit, is, conscious, only, of, this, one, determinateness, as, the, truth, ,, and, then, in, tu, ~, n, of, the, opposite, one, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280173</th>\n",
              "      <td>311570</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>Certainly, we have no knowledge of this inner world as it is here in its immediacy; bu not because Reason is too short ,signted or is limited, or however else onc likes to call it.on this point, we know nothing as yet because we have pot yet gone deep enough~but because of the .simple nature of the matter in hand, that is to say, because in the void nothing is known, or, expressed from theotherside,just because this inner world is determined as the beyond of consciousness.</td>\n",
              "      <td>Certainly, we have no knowledge of this inner world as it is here in its immediacy; bu not because Reason is too short ,signted or is limited, or however else onc likes to call it.on this point, we know nothing as yet because we have pot yet gone deep enough~but because of the .simple nature of the matter in hand, that is to say, because in the void nothing is known, or, expressed from theotherside,just because this inner world is determined as the beyond of consciousness.</td>\n",
              "      <td>477</td>\n",
              "      <td>certainly, we have no knowledge of this inner world as it is here in its immediacy; bu not because reason is too short ,signted or is limited, or however else onc likes to call it.on this point, we know nothing as yet because we have pot yet gone deep enough~but because of the .simple nature of the matter in hand, that is to say, because in the void nothing is known, or, expressed from theotherside,just because this inner world is determined as the beyond of consciousness.</td>\n",
              "      <td>[Certainly, ,, we, have, no, knowledge, of, this, inner, world, as, it, is, here, in, its, immediacy, ;, bu, not, because, Reason, is, too, short, ,, signted, or, is, limited, ,, or, however, else, onc, likes, to, call, it.on, this, point, ,, we, know, nothing, as, yet, because, we, have, pot, yet, gone, deep, enough, ~, but, because, of, the, .simple, nature, of, the, matter, in, hand, ,, that, is, to, say, ,, because, in, the, void, nothing, is, known, ,, or, ,, expressed, from, theothersi...</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280685</th>\n",
              "      <td>312104</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>For tf) think does not mean to be an abstract 'I', bu tan'</td>\n",
              "      <td>For tf) think does not mean to be an abstract 'I', bu tan'</td>\n",
              "      <td>58</td>\n",
              "      <td>for tf) think does not mean to be an abstract 'i', bu tan'</td>\n",
              "      <td>[For, tf, ), think, does, not, mean, to, be, an, abstract, ', I, ', ,, bu, tan, ']</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282193</th>\n",
              "      <td>313710</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>Qf time, bu ra ther when thus tested is over.</td>\n",
              "      <td>Qf time, bu ra ther when thus tested is over.</td>\n",
              "      <td>45</td>\n",
              "      <td>qf time, bu ra ther when thus tested is over.</td>\n",
              "      <td>[Qf, time, ,, bu, ra, ther, when, thus, tested, is, over, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282214</th>\n",
              "      <td>313732</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>The other side is the universal in the form of a tranquil essence; bu tit is for that very reason only something inner which, though not absolutely non existent, still has no reality and can itself become a reality only by getting rid of the individuality which has arrogated reality to itself.</td>\n",
              "      <td>The other side is the universal in the form of a tranquil essence; bu tit is for that very reason only something inner which, though not absolutely non existent, still has no reality and can itself become a reality only by getting rid of the individuality which has arrogated reality to itself.</td>\n",
              "      <td>294</td>\n",
              "      <td>the other side is the universal in the form of a tranquil essence; bu tit is for that very reason only something inner which, though not absolutely non existent, still has no reality and can itself become a reality only by getting rid of the individuality which has arrogated reality to itself.</td>\n",
              "      <td>[The, other, side, is, the, universal, in, the, form, of, a, tranquil, essence, ;, bu, tit, is, for, that, very, reason, only, something, inner, which, ,, though, not, absolutely, non, existent, ,, still, has, no, reality, and, can, itself, become, a, reality, only, by, getting, rid, of, the, individuality, which, has, arrogated, reality, to, itself, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282229</th>\n",
              "      <td>313747</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>like~ wise a common moment, only one that is not present for con~ sciousness as an existent reality, bu as the inner essence of the 'way of the world'.</td>\n",
              "      <td>like~ wise a common moment, only one that is not present for con~ sciousness as an existent reality, bu as the inner essence of the 'way of the world'.</td>\n",
              "      <td>151</td>\n",
              "      <td>like~ wise a common moment, only one that is not present for con~ sciousness as an existent reality, bu as the inner essence of the 'way of the world'.</td>\n",
              "      <td>[like~, wise, a, common, moment, ,, only, one, that, is, not, present, for, con~, sciousness, as, an, existent, reality, ,, bu, as, the, inner, essence, of, the, ', way, of, the, world, ', .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282253</th>\n",
              "      <td>313772</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>The universal is true for the virtuous consciousness in itsfaith, or is implicitly true; tis no yetan actual, bu tan abstract, universality; in this consciousness itselfit is present as a purpose, in the ~way of the world' as an inner principle.</td>\n",
              "      <td>The universal is true for the virtuous consciousness in itsfaith, or is implicitly true; tis no yetan actual, bu tan abstract, universality; in this consciousness itselfit is present as a purpose, in the ~way of the world' as an inner principle.</td>\n",
              "      <td>245</td>\n",
              "      <td>the universal is true for the virtuous consciousness in itsfaith, or is implicitly true; tis no yetan actual, bu tan abstract, universality; in this consciousness itselfit is present as a purpose, in the ~way of the world' as an inner principle.</td>\n",
              "      <td>[The, universal, is, true, for, the, virtuous, consciousness, in, itsfaith, ,, or, is, implicitly, true, ;, tis, no, yetan, actual, ,, bu, tan, abstract, ,, universality, ;, in, this, consciousness, itselfit, is, present, as, a, purpose, ,, in, the, ~way, of, the, world, ', as, an, inner, principle, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284528</th>\n",
              "      <td>316213</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>But also they ough not to be suppressed, bu only to be in conformity with Reason.</td>\n",
              "      <td>But also they ough not to be suppressed, bu only to be in conformity with Reason.</td>\n",
              "      <td>81</td>\n",
              "      <td>but also they ough not to be suppressed, bu only to be in conformity with reason.</td>\n",
              "      <td>[But, also, they, ough, not, to, be, suppressed, ,, bu, only, to, be, in, conformity, with, Reason, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284947</th>\n",
              "      <td>316652</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>If it is altogether in keeping with the station of the individual, without going beyond this station, and ofsuch a nature that the individuality does not possess its station as a character externally attached to it, bu through its own self gives filling to this universali ty, there by showing itself capable ora higher station, then the inner aspect ofthe action is judged to be ambition, andso on.</td>\n",
              "      <td>If it is altogether in keeping with the station of the individual, without going beyond this station, and ofsuch a nature that the individuality does not possess its station as a character externally attached to it, bu through its own self gives filling to this universali ty, there by showing itself capable ora higher station, then the inner aspect ofthe action is judged to be ambition, andso on.</td>\n",
              "      <td>399</td>\n",
              "      <td>if it is altogether in keeping with the station of the individual, without going beyond this station, and ofsuch a nature that the individuality does not possess its station as a character externally attached to it, bu through its own self gives filling to this universali ty, there by showing itself capable ora higher station, then the inner aspect ofthe action is judged to be ambition, andso on.</td>\n",
              "      <td>[If, it, is, altogether, in, keeping, with, the, station, of, the, individual, ,, without, going, beyond, this, station, ,, and, ofsuch, a, nature, that, the, individuality, does, not, possess, its, station, as, a, character, externally, attached, to, it, ,, bu, through, its, own, self, gives, filling, to, this, universali, ty, ,, there, by, showing, itself, capable, ora, higher, station, ,, then, the, inner, aspect, ofthe, action, is, judged, to, be, ambition, ,, andso, on, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285413</th>\n",
              "      <td>317158</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>the Cult itself;justas~ conversely, the essence does not have an actuality devoid ofself, in a rejected people whose subs tance.merely is acknowledged bu tin</td>\n",
              "      <td>the Cult itself;justas~ conversely, the essence does not have an actuality devoid ofself, in a rejected people whose subs tance.merely is acknowledged bu tin</td>\n",
              "      <td>157</td>\n",
              "      <td>the cult itself;justas~ conversely, the essence does not have an actuality devoid ofself, in a rejected people whose subs tance.merely is acknowledged bu tin</td>\n",
              "      <td>[the, Cult, itself;justas~, conversely, ,, the, essence, does, not, have, an, actuality, devoid, ofself, ,, in, a, rejected, people, whose, subs, tance.merely, is, acknowledged, bu, tin]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285788</th>\n",
              "      <td>317559</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>What results from this impoverishment ofSpirit from gettingrid ofthe idea of the community, and its action with regard to its idea, is not the otlon, bu rather bare externality and singularity, the historical mannerofthe manifestationin itsimmediacy and the non spiritual recollection of a supposed individual figure and ofits</td>\n",
              "      <td>What results from this impoverishment ofSpirit from gettingrid ofthe idea of the community, and its action with regard to its idea, is not the otlon, bu rather bare externality and singularity, the historical mannerofthe manifestationin itsimmediacy and the non spiritual recollection of a supposed individual figure and ofits</td>\n",
              "      <td>326</td>\n",
              "      <td>what results from this impoverishment ofspirit from gettingrid ofthe idea of the community, and its action with regard to its idea, is not the otlon, bu rather bare externality and singularity, the historical mannerofthe manifestationin itsimmediacy and the non spiritual recollection of a supposed individual figure and ofits</td>\n",
              "      <td>[What, results, from, this, impoverishment, ofSpirit, from, gettingrid, ofthe, idea, of, the, community, ,, and, its, action, with, regard, to, its, idea, ,, is, not, the, otlon, ,, bu, rather, bare, externality, and, singularity, ,, the, historical, mannerofthe, manifestationin, itsimmediacy, and, the, non, spiritual, recollection, of, a, supposed, individual, figure, and, ofits]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286059</th>\n",
              "      <td>317847</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>However, at this stage, knowledge of the Thing is still not complete; it must be known not only from the standpoint of the immediacy of being and of determinateness, bu also as essence or in rveing, as Self.</td>\n",
              "      <td>However, at this stage, knowledge of the Thing is still not complete; it must be known not only from the standpoint of the immediacy of being and of determinateness, bu also as essence or in rveing, as Self.</td>\n",
              "      <td>207</td>\n",
              "      <td>however, at this stage, knowledge of the thing is still not complete; it must be known not only from the standpoint of the immediacy of being and of determinateness, bu also as essence or in rveing, as self.</td>\n",
              "      <td>[However, ,, at, this, stage, ,, knowledge, of, the, Thing, is, still, not, complete, ;, it, must, be, known, not, only, from, the, standpoint, of, the, immediacy, of, being, and, of, determinateness, ,, bu, also, as, essence, or, in, rveing, ,, as, Self, .]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286235</th>\n",
              "      <td>318027</td>\n",
              "      <td>the phenomenology of spirit</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>Their preservation, regarded from the side of their free existence appearing in the form of con tingency) is His tory; bu regarded from the side of their philom</td>\n",
              "      <td>Their preservation, regarded from the side of their free existence appearing in the form of con tingency) is His tory; bu regarded from the side of their philom</td>\n",
              "      <td>160</td>\n",
              "      <td>their preservation, regarded from the side of their free existence appearing in the form of con tingency) is his tory; bu regarded from the side of their philom</td>\n",
              "      <td>[Their, preservation, ,, regarded, from, the, side, of, their, free, existence, appearing, in, the, form, of, con, tingency, ), is, His, tory, ;, bu, regarded, from, the, side, of, their, philom]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290282</th>\n",
              "      <td>323376</td>\n",
              "      <td>elements of right</td>\n",
              "      <td>hegel</td>\n",
              "      <td>german_idealism</td>\n",
              "      <td>The bu ness of its worship consists in aaions and in dnctrine; for these, it requires posses and property, as well as individuals</td>\n",
              "      <td>The bu ness of its worship consists in aaions and in dnctrine; for these, it requires posses and property, as well as individuals</td>\n",
              "      <td>129</td>\n",
              "      <td>the bu ness of its worship consists in aaions and in dnctrine; for these, it requires posses and property, as well as individuals</td>\n",
              "      <td>[The, bu, ness, of, its, worship, consists, in, aaions, and, in, dnctrine, ;, for, these, ,, it, requires, posses, and, property, ,, as, well, as, individuals]</td>\n",
              "      <td>bu</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  ... word\n",
              "162653      184776  ...   bu\n",
              "203563      229585  ...   bu\n",
              "206754      232922  ...   bu\n",
              "279425      310777  ...   bu\n",
              "279717      311080  ...   bu\n",
              "280033      311425  ...   bu\n",
              "280173      311570  ...   bu\n",
              "280685      312104  ...   bu\n",
              "282193      313710  ...   bu\n",
              "282214      313732  ...   bu\n",
              "282229      313747  ...   bu\n",
              "282253      313772  ...   bu\n",
              "284528      316213  ...   bu\n",
              "284947      316652  ...   bu\n",
              "285413      317158  ...   bu\n",
              "285788      317559  ...   bu\n",
              "286059      317847  ...   bu\n",
              "286235      318027  ...   bu\n",
              "290282      323376  ...   bu\n",
              "\n",
              "[19 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "KOr5K8ukPBDX",
        "outputId": "da33fa21-b9b9-4859-c538-7c2202b2bc1b"
      },
      "source": [
        "fixer_dict = {'\\sderstanding': 'understanding',\r\n",
        "              '\\sject': 'ject',\r\n",
        "              '\\sjects': 'jects', \r\n",
        "              '\\sness': 'ness',\r\n",
        "              '\\wi\\son': 'ion',\r\n",
        "              '\\spri/sori': 'priori',\r\n",
        "              '\\sprincipleb': 'principle',\r\n",
        "              '\\ssciousness': 'sciousness',\r\n",
        "              '\\stion': 'tion',\r\n",
        "              'pri/s': '\\spri',\r\n",
        "              '\\scluding': 'cluding',\r\n",
        "              '\\sdom': 'dom',\r\n",
        "              '\\sers': 'ers',\r\n",
        "              '\\scritiq\\s': '\\scritique\\s'}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-171-d637062f8e12>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    '\\stion', 'tion',\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pVATSBXR-SZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAxx5kYqR-Zo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uoxgBTWR-4H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89zb2qco6Ps_"
      },
      "source": [
        "def glove2dict(glove_filename):\r\n",
        "    with open(glove_filename, encoding='utf-8') as f:\r\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\r\n",
        "        embed = {line[0]: np.array(list(map(float, line[1:])))\r\n",
        "                for line in reader}\r\n",
        "    return embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rer7Z5ucGJcT",
        "outputId": "e4e31aef-c0ad-455d-d80c-88b3954a4c8d"
      },
      "source": [
        "!pip install mittens\r\n",
        "import csv\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "from nltk.corpus import brown\r\n",
        "from mittens import GloVe, Mittens\r\n",
        "from sklearn.feature_extraction import stop_words\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.6/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mittens) (1.19.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4e6C2dmGL18"
      },
      "source": [
        "glove_path = \"/gdrive/MyDrive/Colab_Projects/Phil_NLP/glove.6B.50d.txt\" \r\n",
        "pre_glove = glove2dict(glove_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq1K2Z7YPSRZ"
      },
      "source": [
        "sw = list(stop_words.ENGLISH_STOP_WORDS)\r\n",
        "\r\n",
        "# brown_data = brown.words()[:200000]\r\n",
        "# brown_nonstop = [token.lower() for token in brown_data if (token.lower() not in sw)]\r\n",
        "# oov = [token for token in brown_nonstop if token not in pre_glove.keys()]a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgjx-MBqmLtx",
        "outputId": "932ff560-5f9f-4f55-e47d-41ff27d3b16e"
      },
      "source": [
        "nltk.download('punkt')\n",
        "w2v_data = df['sentence_str'].map(word_tokenize)\n",
        "w2v_data.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "298231    [Hence, a, lessened, demand, for, those, commo...\n",
              "244696                  [However, ,, this, presentation, .]\n",
              "195484    [Thus, minority, under, family, tutelage, ,, a...\n",
              "196485    [The, Meditations, require, such, a, double, r...\n",
              "88851     [A, certain, admixture, ,, accordingly, ,, of,...\n",
              "75492     [But, neither, all, things, nor, out, of, cont...\n",
              "7518      [Well, ,, if, it, depends, on, our, trying, ,,...\n",
              "254883    [This, purposiveness, of, forms, can, be, call...\n",
              "242371    [Vhether, this, unity, is, conceived, as, sum,...\n",
              "203295    [To, possess, ,, to, know, ,, to, grasp, are, ...\n",
              "Name: sentence_str, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJR5pwzS0cP"
      },
      "source": [
        "stopwords_list = stopwords.words('english') + list(string.punctuation) + ['“','”','...',\"''\",'’','``', \"'\", \"‘\"]\r\n",
        "custom_stopwords = ['–', 'also', 'something', 'cf', 'thus', 'two', 'now', 'would', 'make', 'eb', 'u', 'well', 'even', 'said', 'eg', 'us',\r\n",
        "                    'n', 'sein', 'e', 'da', 'therefore', 'however', 'would', 'thing', 'must', 'merely', 'way', 'since', 'latter', 'first',\r\n",
        "                    'B', 'mean', 'upon', 'yet', 'cannot', 'c', 'C', 'let', 'may', 'might', \"'s\", 'b', 'ofthe', 'p.', '_', '-', 'eg', 'e.g.',\r\n",
        "                    'ie', 'i.e.', 'f', 'l', \"n't\", 'e.g', 'i.e', '—', '--', 'hyl', 'phil', 'one', 'press', 'cent', 'place'] + stopwords_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IAcy5iXSa-F"
      },
      "source": [
        "word_list = [item for sublist in list(w2v_data) for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdk93rGbb8ew",
        "outputId": "a0ebe966-e850-496e-e0e1-df936f7e181c"
      },
      "source": [
        "len(word_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9759667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C31xYy0dSh7B"
      },
      "source": [
        "cleaned_words = [x.lower() for x in word_list if x.lower() not in custom_stopwords][:1000000]\r\n",
        "# freq_dist = FreqDist(cleaned_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaKpRPqgkexq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxUVAF_TRmCP"
      },
      "source": [
        "oov = [token for token in cleaned_words if token not in pre_glove.keys()][:1000000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EEyDxIVTh6n"
      },
      "source": [
        "def get_rareoov(xdict, val):\r\n",
        "    return [k for (k,v) in Counter(xdict).items() if v<=val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZtT5NloXiXp"
      },
      "source": [
        "corp_vocab = list(set(oov))\r\n",
        "all_text_doc = [' '.join(cleaned_words)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z2Ahe2Sbhhh",
        "outputId": "c564e846-bae4-4e98-99ab-e75263095072"
      },
      "source": [
        "len(corp_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6295"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiERex-mXkwO",
        "outputId": "1ec5e577-13cc-4983-c2b3-2fc83da7437b"
      },
      "source": [
        "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\r\n",
        "X = cv.fit_transform(all_text_doc)\r\n",
        "Xc = (X.T * X)\r\n",
        "Xc.setdiag(0)\r\n",
        "coocc_ar = Xc.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND9d4zd-Thtt"
      },
      "source": [
        "# oov_rare = get_rareoov(oov, 1)\r\n",
        "# corp_vocab = list(set(oov) - set(oov_rare))\r\n",
        "# tokens = [token for token in cleaned_words if token not in oov_rare]\r\n",
        "# # all_text_doc = [' '.join(tokens)]\r\n",
        "\r\n",
        "# corp_vocab = list(set(oov))\r\n",
        "# all_text_doc = [' '.join(cleaned_words)]\r\n",
        "\r\n",
        "# cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\r\n",
        "# X = cv.fit_transform(all_text_doc)\r\n",
        "# Xc = (X.T * X)\r\n",
        "# Xc.setdiag(0)\r\n",
        "# coocc_ar = Xc.toarray()\r\n",
        "\r\n",
        "mittens_model = Mittens(n=50, max_iter=100)\r\n",
        "\r\n",
        "# new_embeddings = mittens_model.fit(\r\n",
        "#     coocc_ar,\r\n",
        "#     vocab=corp_vocab,\r\n",
        "#     initial_embedding_dict= pre_glove)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfm-vnZBThjX",
        "outputId": "1325ffd9-84ad-4a7f-8f29-adcedccf9785"
      },
      "source": [
        "new_embeddings = mittens_model.fit(\r\n",
        "    coocc_ar,\r\n",
        "    vocab=corp_vocab,\r\n",
        "    initial_embedding_dict= pre_glove)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 100: loss: 14372.193359375"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyrsQIKZrVLw"
      },
      "source": [
        "with open(\"embeddings.txt\", \"w\") as variable_file:\r\n",
        "    variable_file.write(str(new_embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdeTcDdOThXv"
      },
      "source": [
        "newglove = dict(zip(corp_vocab, new_embeddings))\r\n",
        "f = open(\"repo_glove.pkl\",\"wb\")\r\n",
        "pickle.dump(newglove, f)\r\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "9E-VqGqPThKY",
        "outputId": "add81bb6-296c-457c-e5de-048350ce8b01"
      },
      "source": [
        "embeddings_dict = {}\r\n",
        "with open(\"embeddings.txt\", 'r') as f:\r\n",
        "    for line in f:\r\n",
        "        values = line.split()\r\n",
        "        word = values[0]\r\n",
        "        vector = np.asarray(values[1:], \"float32\")\r\n",
        "        embeddings_dict[word] = vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-22cd7d1466d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0membeddings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '...'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-8xOsgzlv-h"
      },
      "source": [
        "def find_closest_embeddings(embedding):\r\n",
        "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PvxH1J_lvxr"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
        "\r\n",
        "glove_file = datapath('/gdrive/MyDrive/Colab_Projects/Phil_NLP/glove.6B.50d.txt')\r\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\r\n",
        "\r\n",
        "_ = glove2word2vec(glove_file, tmp_file)\r\n",
        "\r\n",
        "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_R2UP2MlvjS",
        "outputId": "afce578d-d9cb-4392-aec0-548480348d6c"
      },
      "source": [
        "glove_vectors.most_similar('god')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('divine', 0.8702991604804993),\n",
              " ('heaven', 0.840563178062439),\n",
              " ('christ', 0.8195050954818726),\n",
              " ('faith', 0.7931702733039856),\n",
              " ('allah', 0.7795287370681763),\n",
              " ('holy', 0.772378146648407),\n",
              " ('sacred', 0.7701617479324341),\n",
              " ('true', 0.7637672424316406),\n",
              " ('jesus', 0.7632808685302734),\n",
              " ('gods', 0.7590562105178833)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYhaOPwamVaS"
      },
      "source": [
        "model.get_keras_embedding()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqghkC7QlvXu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE_-CETaR1y0",
        "outputId": "8ae4083d-a4ee-407a-ec80-84c91610df4d"
      },
      "source": [
        "w2v_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [What, new, ,, Socrates, ,, to, make, you, lea...\n",
              "1    [Surely, you, are, not, prosecuting, anyone, b...\n",
              "2    [The, Athenians, do, not, call, this, a, prose...\n",
              "3                        [What, is, this, you, say, ?]\n",
              "4    [Someone, must, have, indicted, you, ,, for, y...\n",
              "Name: sentence_str, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_MK85QoQVor"
      },
      "source": [
        "cleaned_words = [x.lower() for x in all_text_words if x.lower() not in custom_stopwords]\r\n",
        "freq_dist = FreqDist(cleaned_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi0iz6V8WgSN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PJbPzu4WgkG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAybkQa6Wgue"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKncNfMVWg1u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_reHdicWg88"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyBZ1XGFWhEc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IagEBXLwWhLn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Irv6yQpZWhSu",
        "outputId": "f4042b55-0e63-4489-8172-f3cdb710648a"
      },
      "source": [
        "import gensim\r\n",
        "# WORD2VEC\r\n",
        "W2V_SIZE = 300\r\n",
        "W2V_WINDOW = 7\r\n",
        "W2V_EPOCH = 100\r\n",
        "W2V_MIN_COUNT = 2\r\n",
        "# Collect corpus for training word embeddings\r\n",
        "documents = [tokenize(_text) for _text in np.array(train.summary)]\r\n",
        "documents = documents + [tokenize(_text) for _text in np.array(train.title)]\r\n",
        "# Train Word Embeddings and save\r\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW,                                                                                  min_count=W2V_MIN_COUNT)\r\n",
        "w2v_model.build_vocab(documents)\r\n",
        "words = w2v_model.wv.vocab.keys()\r\n",
        "vocab_size = len(words)\r\n",
        "print(\"Vocab size\", vocab_size)\r\n",
        "# Train Word Embeddings\r\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\r\n",
        "w2v_model.save('embeddings.txt')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fa494a565c2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mW2V_MIN_COUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Collect corpus for training word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train Word Embeddings and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzyvja6KWhpg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}